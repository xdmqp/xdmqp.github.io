<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的技术笔记</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xdmqp.github.io/"/>
  <updated>2021-07-06T08:32:12.710Z</updated>
  <id>https://xdmqp.github.io/</id>
  
  <author>
    <name>登高必自</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Java内存泄漏与内存溢出</title>
    <link href="https://xdmqp.github.io/2021/06/30/%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E4%B8%8E%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/"/>
    <id>https://xdmqp.github.io/2021/06/30/%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E4%B8%8E%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/</id>
    <published>2021-06-29T16:00:00.000Z</published>
    <updated>2021-07-06T08:32:12.710Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内存泄漏与内存溢出"><a href="#内存泄漏与内存溢出" class="headerlink" title="内存泄漏与内存溢出"></a>内存泄漏与内存溢出</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>内存泄漏：申请使用完的内存无法释放，导致虚拟机不能再次使用该内存，此时就会产生内存泄露了，因为申请者不用了，而又不能被虚拟机分配给别人用。</p><p>内存溢出：申请的内存超出了JVM能提供的内存大小，此时称之为溢出。</p><p><strong>内存泄漏是内存溢出的一种诱因，但不是唯一因素。</strong></p><h3 id="内存泄漏常见原因"><a href="#内存泄漏常见原因" class="headerlink" title="内存泄漏常见原因"></a>内存泄漏常见原因</h3><h4 id="1-静态集合类"><a href="#1-静态集合类" class="headerlink" title="1.静态集合类"></a>1.静态集合类</h4><p>如HashMap、LinkedList等等。如果这些容器为静态的，那么它们的生命周期与程序一致，则容器中的对象在程序结束之前将不能被释放，从而造成内存泄漏。简单而言，<strong>长生命周期的对象持有短生命周期对象的引用</strong>，<strong>尽管短生命周期的对象不再使用，但是因为长生命周期对象持有它的引用而导致不能被回收。</strong></p><h4 id="2-各种连接，如数据库连接、网络连接和IO连接等"><a href="#2-各种连接，如数据库连接、网络连接和IO连接等" class="headerlink" title="2.各种连接，如数据库连接、网络连接和IO连接等"></a><strong>2.各种连接，如数据库连接、网络连接和IO连接等</strong></h4><p>在对数据库进行操作的过程中，首先需要建立与数据库的连接，当不再使用时，<strong>需要调用close方法来释放与数据库的连接</strong>。只有连接被关闭后，垃圾回收器才会回收对应的对象。否则，如果在访问数据库的过程中，对Connection、Statement或ResultSet<strong>不显性地关闭，将会造成大量的对象无法被回收，从而引起内存泄漏</strong>。</p><h4 id="3-变量不合理的作用域"><a href="#3-变量不合理的作用域" class="headerlink" title="3.变量不合理的作用域"></a>3.变量不合理的作用域</h4><p><strong>一个变量的定义的作用范围大于其使用范围</strong>，很有可能会造成内存泄漏。另一方面，<strong>如果没有及时地把对象设置为null</strong>，很有可能导致内存泄漏的发生。</p><h4 id="4-内部类持有外部类"><a href="#4-内部类持有外部类" class="headerlink" title="4.内部类持有外部类"></a>4.内部类持有外部类</h4><p>如果一个外部类的实例对象的方法返回了一个内部类的实例对象，这个内部类对象被长期引用了，即使那个外部类实例对象不再被使用，但由于内部类持有外部类的实例对象，这个外部类对象将不会被垃圾回收，这也会造成内存泄露。</p><h4 id="5-改变哈希值"><a href="#5-改变哈希值" class="headerlink" title="5.改变哈希值"></a>5.改变哈希值</h4><p>当一个对象被存储进HashSet集合中以后，就不能修改这个对象中的那些参与计算哈希值的字段了，否则，对象修改后的哈希值与最初存储进HashSet集合中时的哈希值就不同了，在这种情况下，即使在contains方法使用该对象的当前引用作为的参数去HashSet集合中检索对象，也将返回找不到对象的结果，这也会导致无法从HashSet集合中单独删除当前对象，造成内存泄露</p><h4 id="6-栈中连续的POP操作"><a href="#6-栈中连续的POP操作" class="headerlink" title="6.栈中连续的POP操作"></a>6.栈中连续的POP操作</h4><p>当进行大量的pop操作时，由于引用未进行置空，gc是不会释放的。如果栈先增长，在收缩，那么从栈中弹出的对象将不会被当作垃圾回收，即使程序不再使用栈中的这些对象，他们也不会回收，因为栈中仍然保存这对象的引用，俗称过期引用，这个内存泄露很隐蔽。</p><h4 id="7-缓存泄漏"><a href="#7-缓存泄漏" class="headerlink" title="7.缓存泄漏"></a>7.缓存泄漏</h4><p>内存泄漏的另一个常见来源是缓存，一旦你把对象引用放入到缓存中，他就很容易遗忘，对于这个问题，可以使用WeakHashMap代表缓存，此种Map的特点是，当除了自身有对key的引用外，此key没有其他引用那么此map会自动丢弃此值。</p><h4 id="8-监听器和回调"><a href="#8-监听器和回调" class="headerlink" title="8.监听器和回调"></a>8.监听器和回调</h4><p>内存泄漏第三个常见来源是监听器和其他回调，如果客户端在你实现的API中注册回调，却没有显示的取消，那么就会积聚。需要确保回调立即被当作垃圾回收的最佳方法是只保存他的若引用，例如将他们保存成为WeakHashMap中的键。</p><h3 id="内存溢出三种情况"><a href="#内存溢出三种情况" class="headerlink" title="内存溢出三种情况"></a>内存溢出三种情况</h3><h4 id="1-堆内存溢出"><a href="#1-堆内存溢出" class="headerlink" title="1.堆内存溢出"></a>1.堆内存溢出</h4><p>java.lang.OutOfMemoryError: Java heap space ——&gt;java堆内存溢出，此种情况最常见，一般<strong>由于内存泄露或者堆的大小设置不当引起</strong>。对于内存泄露，需要通过内存监控软件查找程序中的泄露代码，而<strong>堆大小可以通过虚拟机参数-Xms,-Xmx等修改</strong>。</p><h4 id="2-元空间溢出"><a href="#2-元空间溢出" class="headerlink" title="2.元空间溢出"></a>2.元空间溢出</h4><p>java.lang.OutOfMemoryError: PermGen space 或 java.lang.OutOfMemoryError：MetaSpace ——&gt;java方法区，（java8 元空间）溢出了，<strong>一般出现于程序加载的类过多，或者采用反射、cglib等动态代理生成类技术</strong>的情况，因为<strong>上述情况会产生大量的Class信息存储于方法区</strong>。此种情况可以通过更改方法区的大小来解决，使用类似-XX:PermSize=64m -XX:MaxPermSize=256m的形式修改。另外，<strong>过多的常量尤其是字符串也会导致方法区中的运行时常量池溢出。</strong></p><h4 id="3-栈溢出"><a href="#3-栈溢出" class="headerlink" title="3.栈溢出"></a>3.栈溢出</h4><ul><li><strong>虚拟机栈容量不允许动态扩展</strong>：如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError，一般是<strong>由于程序中存在死循环或者大量递归调用</strong>造成的</li><li><strong>虚拟机栈容量可以动态扩展</strong>：当不断创建线程，如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError　</li></ul><p>可以通过虚拟机参数-Xss来设置栈的大小。</p><h4 id="排查手段"><a href="#排查手段" class="headerlink" title="排查手段"></a>排查手段</h4><p>先通过内存映像工具对Dump出来的堆转储快照进行分析，<strong>重点是确认内存中的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏还是内存溢出。</strong></p><ul><li>如果是内存泄漏，可进一步通过工具<strong>查看泄漏对象到GC Roots的引用链</strong>。这样就能够找到泄漏的对象是<strong>通过怎么样的路径与GC Roots相关联的导致垃圾回收机制无法将其回收</strong>。掌握了泄漏对象的类信息和GC Roots引用链的信息，就可以比较准确地定位泄漏代码的位置。</li><li>如果不存在泄漏，那么就是内存中的对象确实必须存活着，那么此时就需要通过虚拟机的堆参数（ -Xms初始堆空间默认大小和-Xmx最大堆空间默认大小）来适当调大参数；<strong>从代码上检查是否存在某些对象存活时间过长(static修饰)、持有时间过长</strong>的情况，尝试减少运行时内存的消耗。</li></ul><h3 id="如何避免内存泄漏和溢出"><a href="#如何避免内存泄漏和溢出" class="headerlink" title="如何避免内存泄漏和溢出"></a>如何避免内存泄漏和溢出</h3><p>尽早释放无用对象的引用</p><p>使用字符串处理，避免使用String，应大量使用StringBuffer，每一个String对象都得独立占用内存一块区域</p><p>尽量少用静态变量，因为静态变量存放在永久代（方法区），永久代基本不参与垃圾回收</p><p>避免在循环中创建对象</p><p>开启大型文件或从数据库一次拿了太多的数据很容易造成内存溢出，所以在这些地方要大概计算一下数据量的最大值是多少，并且设定所需最小及最大的内存空间值。</p><blockquote><p>内存溢出、内存泄漏和栈溢出：<a href="https://www.cnblogs.com/haimishasha/p/11329510.html" target="_blank" rel="noopener">https://www.cnblogs.com/haimishasha/p/11329510.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内存泄漏与内存溢出&quot;&gt;&lt;a href=&quot;#内存泄漏与内存溢出&quot; class=&quot;headerlink&quot; title=&quot;内存泄漏与内存溢出&quot;&gt;&lt;/a&gt;内存泄漏与内存溢出&lt;/h2&gt;&lt;h3 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="Java" scheme="https://xdmqp.github.io/categories/Java/"/>
    
    
      <category term="Java" scheme="https://xdmqp.github.io/tags/Java/"/>
    
      <category term="内存泄漏" scheme="https://xdmqp.github.io/tags/%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"/>
    
      <category term="内存溢出" scheme="https://xdmqp.github.io/tags/%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/"/>
    
  </entry>
  
  <entry>
    <title>Kafka基础</title>
    <link href="https://xdmqp.github.io/2020/11/24/Kafka%E5%9F%BA%E7%A1%80/"/>
    <id>https://xdmqp.github.io/2020/11/24/Kafka%E5%9F%BA%E7%A1%80/</id>
    <published>2020-11-23T16:00:00.000Z</published>
    <updated>2021-07-06T08:37:26.037Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kafka基础"><a href="#Kafka基础" class="headerlink" title="Kafka基础"></a>Kafka基础</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Kafka 是一个<strong>分布式</strong>的基于<strong>发布/订阅模式</strong>的消息队列（Message Queue），主要应用于大数据实时处理领域。</p><h2 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h2><h3 id="传统消息队列应用场景"><a href="#传统消息队列应用场景" class="headerlink" title="传统消息队列应用场景"></a>传统消息队列应用场景</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201123173826318.png" alt="image-20201123173826318" title="">                </div>                <div class="image-caption">image-20201123173826318</div>            </figure><p><strong>消息队列优点：</strong></p><ol><li>解耦：允许独立扩展或修改两边的处理过程，只需确保它们遵守同样的接口约束</li><li>可恢复性：系统一部分组件失效时，不会影响整个系统。<strong>消息队列降低了进程间的耦合度，即使一个处理消息的进程挂掉，加入队列中的消息扔可以在系统恢复后被处理</strong></li><li>缓冲：有助于控制和优化数据流经过系统的速度，<strong>解决生产消息和消费消息的处理速度不一致的情况</strong></li><li>灵活性&amp;峰值处理能力：<strong>在访问量剧增的情况下</strong>，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。<strong>使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</strong></li><li>异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，<strong>允许用户把一个消息放入队列，但并不立即处理它</strong>。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</li></ol><h3 id="消息队列的两种模式"><a href="#消息队列的两种模式" class="headerlink" title="消息队列的两种模式"></a>消息队列的两种模式</h3><h4 id="点对点模式"><a href="#点对点模式" class="headerlink" title="点对点模式"></a>点对点模式</h4><p>一对一，消费者主动拉取数据，消息收到后立刻从队列中清除</p><p>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。消息被消费以后，queue 中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue 支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201123174556962.png" alt="image-20201123174556962" title="">                </div>                <div class="image-caption">image-20201123174556962</div>            </figure><h4 id="发布订阅模式"><a href="#发布订阅模式" class="headerlink" title="发布订阅模式"></a>发布订阅模式</h4><p>一对多，消费者消费数据后不会清除消息</p><p>消息生产者（发布）将消息发布到 topic 中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic 的消息会被所有订阅者消费。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201123174603211.png" alt="image-20201123174603211" title="">                </div>                <div class="image-caption">image-20201123174603211</div>            </figure><h3 id="Kafka基础架构"><a href="#Kafka基础架构" class="headerlink" title="Kafka基础架构"></a>Kafka基础架构</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201123174638452.png" alt="image-20201123174638452" title="">                </div>                <div class="image-caption">image-20201123174638452</div>            </figure><p><strong>Producer  ：</strong>消息生产者，就是向 kafka broker 发消息的客户端；<br><strong>Consumer</strong>  ：消息消费者，向 kafka broker 取消息的客户端；<br><strong>Consumer Group  （CG ）：</strong>消费者组，由多个 consumer 组成。 消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个 组内 消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即 消费者组是逻辑上的一个订阅者。<br><strong>Broker  ：</strong>一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker可以容纳多个 topic。<br><strong>Topic  ：</strong>可以理解为一个队列， 生产者和消费者面向的都是一个 topic；<br><strong>Partition ：</strong>为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic  可以分为多个 partition，每个 partition 是一个有序的队列；<br><strong>Replica：</strong>副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka 提供了副本机制，一个 topic 的每个分区都有若干个副本，一个 leader 和若干个 follower</p><p><strong>leader ：</strong>每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader。<br><strong>follower ：</strong>每个分区多个副本中的“从”，实时从 leader 中同步数据，保持和 leader 数据的同步。leader 发生故障时，某个 follower 会成为新的 follower</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Kafka基础&quot;&gt;&lt;a href=&quot;#Kafka基础&quot; class=&quot;headerlink&quot; title=&quot;Kafka基础&quot;&gt;&lt;/a&gt;Kafka基础&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://xdmqp.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="https://xdmqp.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Kafka" scheme="https://xdmqp.github.io/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Flume基础</title>
    <link href="https://xdmqp.github.io/2020/11/17/Flume%E5%9F%BA%E7%A1%80/"/>
    <id>https://xdmqp.github.io/2020/11/17/Flume%E5%9F%BA%E7%A1%80/</id>
    <published>2020-11-16T16:00:00.000Z</published>
    <updated>2021-07-06T08:37:15.529Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flume基础"><a href="#Flume基础" class="headerlink" title="Flume基础"></a>Flume基础</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="Flume定义"><a href="#Flume定义" class="headerlink" title="Flume定义"></a>Flume定义</h3><p>Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201117170702468.png" alt="image-20201117170702468" title="">                </div>                <div class="image-caption">image-20201117170702468</div>            </figure><h3 id="Flume基础架构"><a href="#Flume基础架构" class="headerlink" title="Flume基础架构"></a>Flume基础架构</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201117170802490.png" alt="image-20201117170802490" title="">                </div>                <div class="image-caption">image-20201117170802490</div>            </figure><p>Agent是一个JVM进程，以事件的形式将数据从源头送至目的地，由三部分组成：Source、Channel、Sink，事件Event是Flume数据传输的基本单元。Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构，Body 用来存放该条数据，形式为字节数组。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Flume基础&quot;&gt;&lt;a href=&quot;#Flume基础&quot; class=&quot;headerlink&quot; title=&quot;Flume基础&quot;&gt;&lt;/a&gt;Flume基础&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://xdmqp.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flume" scheme="https://xdmqp.github.io/tags/Flume/"/>
    
      <category term="大数据" scheme="https://xdmqp.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Linux文件或目录权限</title>
    <link href="https://xdmqp.github.io/2020/11/17/Linux%E6%96%87%E4%BB%B6%E6%88%96%E7%9B%AE%E5%BD%95%E6%9D%83%E9%99%90/"/>
    <id>https://xdmqp.github.io/2020/11/17/Linux%E6%96%87%E4%BB%B6%E6%88%96%E7%9B%AE%E5%BD%95%E6%9D%83%E9%99%90/</id>
    <published>2020-11-16T16:00:00.000Z</published>
    <updated>2021-07-06T08:37:57.241Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux文件或目录权限"><a href="#Linux文件或目录权限" class="headerlink" title="Linux文件或目录权限"></a>Linux文件或目录权限</h1><h2 id="权限表示"><a href="#权限表示" class="headerlink" title="权限表示"></a>权限表示</h2><p>文件或目录的访问权限有<strong>只读、只写和可执行</strong>三种。以文件为例，只读权限表示只允许读其内容，而禁止对其做任何的更改操作。可执行权限表示允许将该文件作为一个程序执行。文 件被创建时，文件所有者自动拥有对该文件的读、写和可执行权限，以便于对文件的阅读和修改。用户也可根据需要把访问权限设置为需要的任何组合。</p><p>有三种不同类型的用户可对文件或目录进行访问：<strong>文件所有者，同组用户、其他用户</strong>。所有者一般是文件的创建者。所有者可以允许同组用户有权访问文件，还可以将文件的访问权限赋予系统中的其他用户。在这种情况下，系统中每一位用户都能访问该用户拥有的文件或目录。</p><p>每一文件或目录的访问权限都有<strong>三组</strong>，每组用三位表示，分别为：</p><ul><li><strong>文件属主的读、写和执行权限</strong></li><li><strong>与属主同组的用户的读、写和执行权限</strong></li><li><strong>系统中其他用户的读、写和执行权限</strong></li></ul><p>当用<code>ls -l</code>命令显示文件或目录的详细信息时，最左边的一列为文件的访问权限。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l README.txt </span><br><span class="line">-rwxrw-r-- 1 root root 1366 Nov 17 14:48 README.txt</span><br></pre></td></tr></table></figure><p>其中，横线代表空许可，r代表只读，w代表写，x代表可执行。访问权限这里共有10个位置，第一个字符指定了文件类型，如果为“-”代表是文件，如果是“d“代表是一个目录。</p><p>上面的-rwxrw-r–代表README.txt<strong>是一个普通文件，其文件所有者有读、写和执行权限，与文件所有者同组用户具有读写权限，其他用户具有读权限。</strong></p><h2 id="权限修改命令"><a href="#权限修改命令" class="headerlink" title="权限修改命令"></a>权限修改命令</h2><h3 id="chmod命令"><a href="#chmod命令" class="headerlink" title="chmod命令"></a>chmod命令</h3><p>用于改变文件或目录的访问权限</p><h4 id="1-文字设定法"><a href="#1-文字设定法" class="headerlink" title="1.文字设定法"></a><strong>1.文字设定法</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod ［who］ ［+ | – | =］ ［mode］ 文件名</span><br></pre></td></tr></table></figure><p>who可以是下述字母中任一个或者它们的组合：</p><ul><li>u 表示“用户（user）”，即文件或目录的所有者。</li><li>g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户。</li><li>o 表示“其他（others）用户”。</li><li>a 表示“所有（all）用户”。它是系统默认值。</li></ul><p>操作符号可以是：</p><ul><li>+ 添加某个权限。</li><li>– 取消某个权限。</li><li>= 赋予给定权限并取消其他所有权限（如果有的话）。</li><li>设置mode所表示的权限可用下述字母的任意组合：</li><li>r 可读。</li><li>w 可写。</li><li>x 可执行。</li><li>X 只有目标文件对某些用户是可执行的或该目标文件是目录时才追加x 属性。</li><li>s 在文件执行时把进程的属主或组ID置为该文件的文件属主。方式“u＋s”设置文件的用户ID位，“g＋s”设置组ID位。</li><li>t 保存程序的文本到交换设备上。</li><li>u 与文件属主拥有一样的权限。</li><li>g 与和文件属主同组的用户拥有一样的权限。</li><li>o 与其他用户拥有一样的权限。</li><li>-c : 若该档案权限确实已经更改，才显示其更改动作</li><li>-f : 若该档案权限无法被更改也不要显示错误讯息</li><li>-v : 显示权限变更的详细资料</li><li>-R : 对目前目录下的所有档案与子目录进行相同的权限变更(即以递回的方式逐个变更)</li><li>–help : 显示辅助说明</li><li>–version : 显示版本</li></ul><p>文件名：以空格分开的要改变权限的文件列表，支持通配符。在一个命令行中可给出多个权限方式，其间用逗号隔开。</p><p>例如：chmod g+r，o+r example使同组和其他用户对文件example 有读权限。</p><h4 id="2-数字设定法"><a href="#2-数字设定法" class="headerlink" title="2.数字设定法"></a>2.数字设定法</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod ［mode］ 文件名</span><br></pre></td></tr></table></figure><p><strong>首先要了解数字所表示的权限属性的含义：0表示没有权限，1表示可执行权限，2表示可写权限，4表示可读权限，然后将其相加。因此mode由三位0到7的数字组成，分别表示文件所有者、所有者同组用户和其他用户的权限。</strong></p><p>例如 chmod 750 test.txt的含义为：文件所有者具有读、写和执行权限，所有者同组用户具有读和执行权限，其他用户没有任何权限。</p><h3 id="chgrp命令"><a href="#chgrp命令" class="headerlink" title="chgrp命令"></a>chgrp命令</h3><p>改变文件或目录所属的组</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chgrp ［option］ group filename</span><br></pre></td></tr></table></figure><p><strong>参数：</strong></p><ul><li>-c或–changes 效果类似”-v”参数，但仅回报更改的部分。</li><li>-f或–quiet或–silent 　不显示错误信息。</li><li>-h或–no-dereference 　只对符号连接的文件作修改，而不更动其他任何相关文件。</li><li>-R或–recursive 　递归处理，将指定目录下的所有文件及子目录一并处理。</li><li>-v或–verbose 　显示指令执行过程。</li><li>–help 　在线帮助。</li><li>–reference=&lt;参考文件或目录&gt; 　把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同。</li><li>–version 　显示版本信息。</li></ul><p>该命令改变指定指定文件所属的用户组。<strong>其中group可以是用户组ID，也可以是/etc/group文件中用户组的组名</strong>。文件名是以空格分开的要改变属组的文件列表，支持通配符。<strong>如果用户不是该文件的属主或超级用户，则不能改变该文件的组。</strong></p><h3 id="chown命令"><a href="#chown命令" class="headerlink" title="chown命令"></a>chown命令</h3><p>更改某个文件或目录的属主和属组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown [option] user或group file</span><br></pre></td></tr></table></figure><p>命令将指定文件的拥有者改为指定的用户或组。<strong>用户可以是用户名或用户ID。组可以是组名或组ID</strong>。文件是以空格分开的要改变权限的文件列表，支持通配符。</p><p><strong>参数：</strong></p><ul><li>user : 新的档案拥有者的使用者 ID</li><li>group : 新的档案拥有者的使用者群体(group)</li><li>-c : 若该档案拥有者确实已经更改，才显示其更改动作</li><li>-f : 若该档案拥有者无法被更改也不要显示错误讯息</li><li>-h : 只对于连结(link)进行变更，而非该 link 真正指向的档案</li><li>-v : 显示拥有者变更的详细资料</li><li>-R : 对目前目录下的所有档案与子目录进行相同的拥有者变更(即以递回的方式逐个变更)</li><li>–help : 显示辅助说明</li><li>–version : 显示版本</li></ul><blockquote><p><strong><a href="https://blog.csdn.net/strivenoend/article/details/80446603" target="_blank" rel="noopener">https://blog.csdn.net/strivenoend/article/details/80446603</a></strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Linux文件或目录权限&quot;&gt;&lt;a href=&quot;#Linux文件或目录权限&quot; class=&quot;headerlink&quot; title=&quot;Linux文件或目录权限&quot;&gt;&lt;/a&gt;Linux文件或目录权限&lt;/h1&gt;&lt;h2 id=&quot;权限表示&quot;&gt;&lt;a href=&quot;#权限表示&quot; cla
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://xdmqp.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://xdmqp.github.io/tags/Linux/"/>
    
      <category term="文件目录" scheme="https://xdmqp.github.io/tags/%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>信息论中的熵</title>
    <link href="https://xdmqp.github.io/2020/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%9A%84%E7%86%B5/"/>
    <id>https://xdmqp.github.io/2020/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%9A%84%E7%86%B5/</id>
    <published>2020-08-17T16:00:00.000Z</published>
    <updated>2020-08-18T10:19:43.334Z</updated>
    
    <content type="html"><![CDATA[<h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p>用来表示事件所含信息量的大小。熵越大，所含信息量越大</p><h4 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h4><p>假设有随机变量$X$，其取值有$x_1,x_2,…$，</p><p>每个事件发生的概率为：$P(x_i) = P(X=x_i)$，<strong>当事件发生的概率越小时，就认为该事件的信息量就越大</strong>。</p><p>信息量的计算公式为：<br>$$<br>I(x_i) = -logP(x_i)<br>$$</p><h4 id="熵-1"><a href="#熵-1" class="headerlink" title="熵"></a>熵</h4><p>熵的具体计算方式为<strong>各类事件信息量的期望值</strong>，计算公式如下：<br>$$<br>H(X) = \sum P(x_i)I(x_i) = -\sum P(x_i)logP(x_i)<br>$$</p><h3 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h3><p>相对熵又称<strong>KL散度</strong>，一般用来计算<strong>两个分布间的差异</strong></p><p>离散变量A和B分布的差异计算公式为：<br>$$<br>D_{KL}(A | B)=\sum P_{A}(x_i) \log (\frac{P_{A}(x_i)}{P_{B}(x_i)})=\sum P_{A}(x_i) \log (P_{A}(x_i))-\sum P_{A}(x_{i}) \log (P_{B}(x_{i}))<br>$$<br>连续变量A和B分布的差异计算公式为：<br>$$<br>D_{K L}(A | B)=\int a(x) \log \left(\frac{a(x)}{b(x)}\right)<br>$$<br>从公式中可以看出：</p><ol><li>当$P(A)=P(B)$时，即两个变量的分布完全相同，此时KL散度为0</li><li>离散事件分布相对熵计算公式中减号前的部分恰好是A的熵的相反数</li><li>$D_{KL}(A||B) \ne D_{KL}(B||A)$</li></ol><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>交叉熵的计算公式为：<br>$$<br>H(A, B) = -\sum P_A(x_i)logP_B(x_i)<br>$$<br>从前面相对熵的计算公式中可以看出：<br>$$<br>D_{KL}(A||B) = -H(A) + H(A, B)<br>$$<br>即相对熵=-熵+交叉熵，当熵固定的时候，用相对熵衡量两个分布的差异时，等价于用交叉熵来衡量。<strong>而交叉熵的运算更为简单，因此交叉熵更适合用来当做代价。</strong></p><blockquote><p><strong>参考资料</strong></p><p>从熵到交叉熵损失函数的理解：<a href="https://www.cnblogs.com/Elaine-DWL/p/11255522.html" target="_blank" rel="noopener">https://www.cnblogs.com/Elaine-DWL/p/11255522.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;熵&quot;&gt;&lt;a href=&quot;#熵&quot; class=&quot;headerlink&quot; title=&quot;熵&quot;&gt;&lt;/a&gt;熵&lt;/h3&gt;&lt;p&gt;用来表示事件所含信息量的大小。熵越大，所含信息量越大&lt;/p&gt;
&lt;h4 id=&quot;信息量&quot;&gt;&lt;a href=&quot;#信息量&quot; class=&quot;headerli
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://xdmqp.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>基于模型的强化学习方法</title>
    <link href="https://xdmqp.github.io/2020/08/10/%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <id>https://xdmqp.github.io/2020/08/10/%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</id>
    <published>2020-08-09T16:00:00.000Z</published>
    <updated>2021-07-06T08:29:21.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基于模型-Model-Based-的强化学习方法"><a href="#基于模型-Model-Based-的强化学习方法" class="headerlink" title="基于模型(Model-Based)的强化学习方法"></a>基于模型(Model-Based)的强化学习方法</h2><h3 id="方法引入"><a href="#方法引入" class="headerlink" title="方法引入"></a>方法引入</h3><p>前面所说的基于价值（Q-Learning，DQN等）和基于策略（Policy Gradient、Actor-Critic、A3C等）的强化学习方法都不是基于模型的，<strong>它们都是从与环境的交互获得的经验中学习，而基于模型的强化学习方法则是尝试从对环境建立的模型中学习</strong>，一般是有两个相互独立的模型：</p><ol><li>状态转换预测模型：输入当前状态$s$和动作$a$，预测下一个状态$s’$</li></ol><p>$$<br>S_{t+1} \sim P(S_{t+1}|S_t,A_t)<br>$$</p><ol start="2"><li>奖励预测模型：输入当前状态$s$和动作$a$，预测环境的奖励$r$</li></ol><p>$$<br>R_{t+1} \sim R(R_{t+1}|S_t,A_t)<br>$$</p><p>基于模型的强化学习的思路如下图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/Model-Based.png" alt="Model-Based" title="">                </div>                <div class="image-caption">Model-Based</div>            </figure><h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><p>假设有若干组如下的训练数据：<br>$$<br>S_1,A_1,R_2,S_2,A_2,R_2,…,S_T<br>$$<br>对于每组经历，我们可以将其转化为$T-1$组训练样本：<br>$$<br>S_1,A_1 \to S_2,;S_1,A_1 \to R_2<br>$$</p><p>$$<br>S_2,A_2 \to S_3,;S_2,A_2 \to R_3<br>$$</p><p>$$<br>……<br>$$</p><p>$$<br>S_{T-1},A_{T-1} \to S_T,;S_{T-1},A_{T-1} \to R_T<br>$$</p><p>这样，左边的训练样本构成了一个分类模型训练集，输入动作和状态，输出下一个状态，右边的训练样本构成了一个回归模型训练集，输入动作和状态，输出奖励。</p><p>基于模型的强化学习方法有<strong>思路清晰、无需和环境持续交互</strong>等优点，但是<strong>在实际应用中，我们建立的模型大多不能准确描述真正的环境转化模型</strong>，这时基于模型预测的效果就不太理想了。 于是就诞生了Dyna算法框架：将Model-Based和Model-Free方法结合起来，取长补短。</p><h3 id="Dyna算法框架"><a href="#Dyna算法框架" class="headerlink" title="Dyna算法框架"></a>Dyna算法框架</h3><p>Dyna将Model-Based和Model-Free方法结合起来，既从模型中学习，也从和环境交互的经历中学习，从而更新价值函数或策略函数，Dyna的设计思路如下图所示，可以看出Dyna在Model-Based方法的基础上增加了”Direct RL”过程，也就是Model-Free方法的思路。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/Dyna.png" alt="Dyna算法思路" title="">                </div>                <div class="image-caption">Dyna算法思路</div>            </figure><p>Dyna算法框架和不同的Model-Free方法结合到一起，可以得到不同的算法。如果使用基于价值的Q-Learning，就得到Dyna-Q算法。</p><h3 id="Dyna-Q算法流程"><a href="#Dyna-Q算法流程" class="headerlink" title="Dyna-Q算法流程"></a>Dyna-Q算法流程</h3><p>下面给出基于价值函数的Dyna-Q算法的流程，假设模型使用的是查表法：</p><ol><li><p>初始化任意一个状态$s$，任意一个动作$a$对应的状态价值$Q(s,a)$，初始化奖励模型$R(s,a)$和状态模型$P(s,a)$</p></li><li><p>for i=1 to最大迭代次数T:</p><p>a) $S \gets current \quad state$</p><p>b) $A \gets \epsilon-greedy(S,Q)$</p><p>c) 执行动作$A$，得到新状态$S’$和奖励$R$</p><p>d) Q-Learning更新价值函数：$Q(S,A) =Q(S,A) + \alpha[R +\gamma\max_aQ(S’,a) -Q(S,A)]$</p><p>e) 使用$S,A,S’$更新模型$P(s,a)$，使用$S,A,R$更新模型$R(s,a)$</p><p>f) for j=1 to 最大次数n:</p><p>​    随机选择一个之前出现过的状态$S$，在状态$S$上出现过的动作随机选择一个动作$A$</p><p>​    基于模型$P(S,A)$得到$S’$，基于模型$R(S,A)得到$R$</p><p>​    使用Q-Learning更新价值函数：$Q(S,A) =Q(S,A) + \alpha[R +\gamma\max_aQ(S’,a) -Q(S,A)]$</p></li></ol><p>从流程中可以看出，Dyna框架在每次迭代中，会先和环境交互，并更新价值函数或策略函数，接着进行n次基于模型的预测，同样更新价值函数或策略函数，实现了上述的Dyna算法思路。</p><h3 id="Dyna-2算法框架"><a href="#Dyna-2算法框架" class="headerlink" title="Dyna-2算法框架"></a>Dyna-2算法框架</h3><p>在Dyna算法框架的基础上后来又发展出了Dyna-2算法框架。和Dyna相比，Dyna-2将和和环境交互的经历以及模型的预测这两部分使用进行了分离。还是以Q函数为例，Dyna-2将记忆分为永久性记忆（permanent memory）和瞬时记忆（transient memory）, 其中永久性记忆利用实际的经验来更新，瞬时记忆利用模型模拟经验来更新。</p><p>永久性记忆的Q函数定义为：<br>$$<br>Q(S,A) = \phi(S,A)^T\theta<br>$$<br>瞬时记忆的Q函数定义为：<br>$$<br>Q’(S,A) = \overline{\phi}(S,A)^T\overline{\theta }<br>$$<br>组合后记忆的Q函数为：<br>$$<br>\overline{Q}(S,A) = \phi(S,A)^T\theta + \overline{\phi}(S,A)^T\overline{\theta }<br>$$<br>Dyna-2的基本思想是在选择实际的执行动作前，智能体先执行一遍从当前状态开始的基于模型的模拟，该模拟将仿真完整的轨迹，以便评估当前的动作值函数。智能体会根据模拟得到的动作值函数加上实际经验得到的值函数共同选择实际要执行的动作。价值函数的更新方式类似Sarsa(λ)。算法流程如下：</p><img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/Dyna-2.png" alt="Dyna-2" /><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>基于模型的强化学习方法一般不单独使用，而是和不基于模型的强化学习结合起来，而Dyna算法框架就是常用的实现方式。对于模型部分，可以用查表和监督学习等方法，预测或者采样得到模拟的经历。对于非模型部分，使用价值函数或者策略函数近似表示。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基于模型-Model-Based-的强化学习方法&quot;&gt;&lt;a href=&quot;#基于模型-Model-Based-的强化学习方法&quot; class=&quot;headerlink&quot; title=&quot;基于模型(Model-Based)的强化学习方法&quot;&gt;&lt;/a&gt;基于模型(Model-Bas
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://xdmqp.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Model-Based RL" scheme="https://xdmqp.github.io/tags/Model-Based-RL/"/>
    
  </entry>
  
  <entry>
    <title>强化学习 Deep Deterministic Policy Gradient</title>
    <link href="https://xdmqp.github.io/2020/08/06/Deep%20Deterministic%20Policy%20Gradient/"/>
    <id>https://xdmqp.github.io/2020/08/06/Deep%20Deterministic%20Policy%20Gradient/</id>
    <published>2020-08-05T16:00:00.000Z</published>
    <updated>2021-07-06T06:47:57.473Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deep-Deterministic-Policy-Gradient"><a href="#Deep-Deterministic-Policy-Gradient" class="headerlink" title="Deep Deterministic Policy Gradient"></a>Deep Deterministic Policy Gradient</h2><h3 id="随机策略和确定性策略"><a href="#随机策略和确定性策略" class="headerlink" title="随机策略和确定性策略"></a>随机策略和确定性策略</h3><p>先来说一下Deterministic Policy Gradient，也就是确定性策略。顾名思义，确定性策略是相对于随机性策略而言的。对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样的动作空间维度极大。如果采用随机策略研究所有可能动作的概率并计算各个动作的价值，需要采样的数据量较大，学习也会比较慢，这时候确定性策略就是更好的方式啦。</p><p>对于随机策略，在相同的状态下输出的动作是不唯一的，而是满足一定的概率分布；对于确定性策略，在相同状态下利用确定性梯度优化策略，其输出的动作是确定的。</p><h3 id="DPG到DDPG"><a href="#DPG到DDPG" class="headerlink" title="DPG到DDPG"></a>DPG到DDPG</h3><p>先来看下基于Q值的随机策略梯度的梯度计算公式<br>$$<br>\nabla_{\theta}J(\pi_{\theta}) = E_{s\sim\rho^{\pi}, a\sim\pi_{\theta}}[\nabla_{\theta}log \pi_{\theta}(s,a)Q_{\pi}(s,a)]<br>$$<br>其中，$\rho^{\pi}$是状态的采样空间，$\nabla_{\theta}log \pi_{\theta}(s,a)$是分值函数，可以看出随机策略梯度需要在整个动作空间$\pi_\theta$进行采样。</p><p>而DPG基于Q值的梯度计算公式为：<br>$$<br>\nabla_{\theta}J(\pi_{\theta}) = E_{s\sim\rho^{\pi}}[\nabla_{\theta} \pi_{\theta}(s)\nabla_{a}Q_{\pi}(s,a)|<em>{a=\pi</em>{\theta}(s)}]<br>$$<br>与随机策略梯度相比，少了对动作的积分，多了Q函数对动作的倒数</p><p>从DPG到DDPG得过程可以类比DQN到DDQN的过程，将网络结构分为当前网络和目标网络。因为DPG本身就有Actor网络和Critic两个网络，所以改进后就有四个网络：<strong>Actor目标网络、Actor当前网络、Critic目标网络、Critic当前网络</strong>。其中，两个Actor网络的结构相同，两个Critic网络的结构相同。</p><h3 id="DDPG原理"><a href="#DDPG原理" class="headerlink" title="DDPG原理"></a>DDPG原理</h3><p><strong>Actor当前网络</strong>：根据当前状态$S$选择当前动作$A$，获得状态$S’$和奖励$R$；负责迭代更新策略网络参数$\theta$</p><p><strong>Actor目标网络</strong>：经验回放池采样的下一状态$S’$使用贪婪法选择动作$A’$，网络参数$\theta’$定期从$\theta$复制</p><p><strong>Critic当前网络</strong>：计算当前Q值$Q(S,A,w)$；负责迭代更新价值网络参数$w$，目标Q值$y_i = R+\gamma Q’(S’,A’,w’)$</p><p><strong>Critic目标网络</strong>：基于经验回放池和Actor目标网络提供的$S’,A’$计算目标Q值中的$Q’(S’,A’,w’)$部分，网络参数$w’$定期从$w$复制</p><p>DDPG从当前网络到目标网络的复制与DDQN不一样，DDQN直接将当前Q网络的参数复制到目标Q网络，即$w’=w$。DDPG没有采用这种硬更新方式，而是<strong>采用软更新</strong>，即每次参数只更新一点点：<br>$$<br>w’ \gets \tau w+ (1-\tau)w’<br>$$</p><p>$$<br>\theta’ \gets \tau \theta+ (1-\tau)\theta’<br>$$</p><p>其中$\tau$为更新系数，一般取值较小，如0.1或者0.01</p><p>为了学习过程增加一些随机性，增加学习的覆盖面，DDPG会对选出来的动作$A$增加一定的噪声\mathcal{N}，即：<br>$$<br>A = \pi_{\theta}(S) + \mathcal{N}<br>$$<br>然后是DDPG的损失函数，首先是Critic当前网络，<strong>仍然是采用均方误差</strong>：<br>$$<br>J(w) =\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2<br>$$<br>但是<strong>由于是确定性策略，Actor当前网络的损失函数就和之前的PG，A3C不同了</strong>:<br>$$<br>\nabla_J(\theta) = \frac{1}{m}\sum\limits_{j=1}^m[\nabla_{a}Q_(s_i,a_i,w)|<em>{s=s_i,a=\pi</em>{\theta}(s)}\nabla_{\theta} \pi_{\theta(s)}|<em>{s=s_i}]<br>$$<br>因为采用确定性策略梯度，所以当对同一状态，如果输出两个不同的动作$a_1$,$a_2$，则在Critic当前网络中会得到两个Q值：$Q_1,Q_2$。这时候就比较两者，如果$Q_1&gt;Q_2$，就增加$a_1$的概率，降低$a_2$的概率，因为Actor想要得到更大的Q值。所以Actor的损失可以理解为得到的Q值越大损失越小，Q值越小损失越大，因此只要对Critic当前网络返回的Q值加个负号即可：<br>$$<br>J(\theta) =  -\frac{1}{m}\sum\limits</em>{j=1}^m Q_(s_i,a_i,w)<br>$$</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>输入：<strong>Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络,参数分别为$\theta,\theta′,w,w′$,衰减因子$\gamma$, 软更新系数$\tau$,批量梯度下降的样本数$m$,目标Q网络参数更新频率$C$,最大迭代次数$T$,随机噪音函数$\mathcal{N}$</strong></p><p>输出：<strong>最优Actor当前网络参数$\theta$,Critic当前网络参数$w$</strong></p><ol><li><p>随机初始化$\theta,w,w’=w,\theta’=theta$,清空经验回放的集合D</p></li><li><p>for i form 1 to T:</p><p>1) 初始化$S$为当前状态序列的第一个状态，拿到其特征向量$\phi(S)$</p><p>2) 在Actor当前网络基于状态$S$得到动作$A =\pi_{\theta}(\phi(S)) + \mathcal{N}$</p><p>3) 执行动作A，得到新状态$S’$，奖励$R$，是否终止状态$is_end$</p><p>4) 将${\phi(S),A,R,\phi(S’),is_end}$这个五元组存入经验回放集合$D$</p><p>5) $S=S’$</p><p>6) 从经验回放集合$D$中采样$m$个样本${\phi(S_j),A_j,R_j,\phi(S’<em>j),is_end_j}, j=1,2.,,,m$，计算目标Q值$y_j$：<br>$$<br>y_j= \begin{cases} R_j&amp; {is_end_j; is ;true}\ R_j + \gamma Q’(\phi(S’_j),\pi</em>{ \theta’}(\phi(S’<em>j)),w’)&amp; {is_end_j; is ;false} \end{cases}<br>$$<br>7) 使用均方误差损失函数$\frac{1}{m}\sum\limits</em>{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络反向传播更新Critic当前网络的所有参数$w$</p><p>8) 使用$J(\theta) =  -\frac{1}{m}\sum\limits_{j=1}^m Q_(s_i,a_i,\theta)$，通过神经网路反向传播更新Actor当前网络的所有参数$\theta$</p><p>9) $if\quaadT%C==1$，更新Critic目标网络和Actor目标网络参数：<br>$$<br>w’ \gets \tau w+ (1-\tau)w’<br>$$</p></li></ol><p>$$<br>\theta’ \gets \tau \theta+ (1-\tau)\theta’<br>$$</p><p>​      10) 如果$S’$是终止状态，当前轮迭代完毕，否则转到步骤2)</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>DDPG借鉴了DDQN的思想，通过增加双网络和经验回放，以及对Actor梯度误差损失函数的改进，比较好的解决了Actor-Critic难以收敛的问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Deep-Deterministic-Policy-Gradient&quot;&gt;&lt;a href=&quot;#Deep-Deterministic-Policy-Gradient&quot; class=&quot;headerlink&quot; title=&quot;Deep Deterministic Polic
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://xdmqp.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy-Based RL" scheme="https://xdmqp.github.io/tags/Policy-Based-RL/"/>
    
      <category term="Deep Deterministic Policy Gradient" scheme="https://xdmqp.github.io/tags/Deep-Deterministic-Policy-Gradient/"/>
    
  </entry>
  
  <entry>
    <title>A3C</title>
    <link href="https://xdmqp.github.io/2020/08/05/A3C/"/>
    <id>https://xdmqp.github.io/2020/08/05/A3C/</id>
    <published>2020-08-04T16:00:00.000Z</published>
    <updated>2021-07-06T03:09:27.136Z</updated>
    
    <content type="html"><![CDATA[<h2 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h2><h3 id="算法引入"><a href="#算法引入" class="headerlink" title="算法引入"></a>算法引入</h3><p>A3C的改进方案类似于DQN中的经验池的思想，并且克服了DQN经验池的弊端：<strong>经验池内的数据之间相关性太强，用于训练的效果可能不佳</strong>，并且实现了异步并发的学习模型。具体就是利用多线程的方法，同时在多个线程里分别和环境进行交互学习，然后把每个线程学习到的经验保存在一个公共空间里，并且定期从公共空间里取出这些线程学习到的经验，指导自身和环境之间的学习交互。</p><h3 id="算法优化"><a href="#算法优化" class="headerlink" title="算法优化"></a>算法优化</h3><p>相比Actor-Critic，A3C的优化主要有三点：<strong>异步训练框架、网络结构优化和Critic评估点的优化</strong>。</p><h4 id="异步训练框架"><a href="#异步训练框架" class="headerlink" title="异步训练框架"></a>异步训练框架</h4><p>如上图所示，Global Network就相当于公共空间，就是一个公共的神经网络模型，该模型包括Actor网络和Critic网络两部分的功能。公共模型下有n个Worker线程，每个线程里有和公共神经网络一样的网路结构，每个线程都能相互独立地和环境进行交互获得经验数据。</p><p>每个线程与环境交互一定量的数据后，就计算当前线程里神经网络损失函数的梯度，不过计算出来的梯度并<strong>不是用来更新自己线程里的神经网络，而是去更新公共神经网络</strong>。每隔一段时间，线程会将自身的神经网络参数更新为公共神经网络的参数。</p><p>因此，公共神经网络就是我们要学习的模型，线程里的网络模型主要用于和环境交互，拿到高质量的数据来帮助模型更快收敛。</p><h4 id="网络结构优化"><a href="#网络结构优化" class="headerlink" title="网络结构优化"></a>网络结构优化</h4><p>Actor-Critic中<strong>使用了两个不同的网络Actor和Critic，而A3C中又将两个网络放到了一起</strong>，即输入状态$S$，输出状态价值$V$，和策略$\pi$。</p><h4 id="Critic评估点优化"><a href="#Critic评估点优化" class="headerlink" title="Critic评估点优化"></a>Critic评估点优化</h4><p>A3C采用了优势函数$A$作为Critic评估点，优势函数$A$在时刻t不考虑参数的默认表达式为：<br>$$<br>A(S,A,t) = Q(S,A) - V(S)<br>$$<br>其中，$Q(S,A)$通过N步采样近似估计，以加速收敛：<br>$$<br>Q(S,A) = R_t +   \gamma R_{t+1} +…\gamma^{n-1} R_{t+n-1}<br>$$<br>$V(S)$的值则需要通过Critic网络学习得到</p><p>所以，A3C中的优势函数表达为：<br>$$<br>A(S,t) = R_t +  \gamma R_{t+1} +…\gamma^{n-1} R_{t+n-1} + \gamma^n V(S’) - V(S)<br>$$<br>对于Actor和Critic的损失函数部分，加入了策略$\pi$的熵项，系数为c，即策略参数的梯度更新公式变成了：<br>$$<br>\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)A(S,t) + c\nabla_{\theta}H(\pi(S_t, \theta))<br>$$</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>以A3C中任意一个线程的算法流程为例：</p><p>输入：<strong>公共部分的A3C神经网络结构，对应参数$\theta,w$，本线程的神经网络结构，对应参数$\theta ‘, w’$，全局共享的迭代轮数T，全局最大迭代次数$T_max$，线程内单次迭代时间序列最大长度$\T_local$，状态特征维度$n$，动作集合$A$，步长$\alpha,\beta$，熵系数$c$，衰减因子$\gamma$</strong></p><p>输出：<strong>公共部分的A3C神经网络参数$\theta,w$</strong></p><ol><li><p>更新时间序列$t=1$</p></li><li><p>重置Actor和Critic的梯度更新量：$d\theta \gets 0, dw\gets 0$</p></li><li><p>将公共部分的A3C神经网络参数同步到本线程神经网络：$\theta ‘ =\theta,;; w’=w$</p></li><li><p>$t_{start} = t$，初始化状态$s_t$</p></li><li><p>基于策略$\pi(a_t|s_t;\theta)$选择动作$a_t$</p></li><li><p>执行动作$a_t$得到奖励$r_t$和新状态$s_{t+1}$</p></li><li><p>$t \gets t+1, T \gets T+1$</p></li><li><p>如果$s_t$是终止状态或者$t-t_{start} == t_{local}$，进入步骤9，否则进入步骤5</p></li><li><p>计算最后一个时间序列位置$s_t$的$Q(s,t)$：</p></li></ol><p>$$<br>Q(s,t)= \begin{cases} 0&amp; {terminal;state}\ V(s_t,w’)&amp; {none;terminal;state,bootstrapping} \end{cases}<br>$$</p><ol start="10"><li><p>$for \quad i \in (t-1,t-2,…t_{start}):$</p><p>       a) 计算每个时刻的$Q(s,i)$：$Q(s,i) = r_i + \gamma Q(s,i+1)$</p><p>       b) 累计Actor的本地梯度更新：<br>$$<br>d\theta \gets d\theta + \nabla_{\theta ‘}log \pi_{\theta’}(s_i,a_i)(Q(s,i)-V(S_i, w’)) + c\nabla_{\theta ‘}H(\pi(s_i, \theta ‘))<br>$$<br>       c) 累计Critic的本地梯度更新：<br>$$<br>dw  \gets dw + \frac{\partial (Q(s,i)-V(S_i, w’))^2}{\partial w’}<br>$$</p></li><li><p>更新全局神经网路的模型参数：</p></li></ol><p>$$<br>\theta = \theta -\alpha d\theta,;w = w -\beta dw<br>$$</p><ol start="12"><li>如果$T &gt; T_{max}$，算法结束，输出公共神经网络参数$\theta,w$，否则进入步骤3</li></ol><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>A3C算法解决了Actor-Critic难以收敛的问题，更重要的一点就是提出了一种通用的异步并发的强化学习框架，其他强化学习算法也同样适用，这是A3C最大的贡献。</p><blockquote><p>强化学习（十五）A3C：<a href="https://www.cnblogs.com/pinard/p/10334127.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/10334127.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;A3C&quot;&gt;&lt;a href=&quot;#A3C&quot; class=&quot;headerlink&quot; title=&quot;A3C&quot;&gt;&lt;/a&gt;A3C&lt;/h2&gt;&lt;h3 id=&quot;算法引入&quot;&gt;&lt;a href=&quot;#算法引入&quot; class=&quot;headerlink&quot; title=&quot;算法引入&quot;&gt;&lt;/a&gt;算法引
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://xdmqp.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="A3C" scheme="https://xdmqp.github.io/tags/A3C/"/>
    
      <category term="Policy-Based RL" scheme="https://xdmqp.github.io/tags/Policy-Based-RL/"/>
    
  </entry>
  
  <entry>
    <title>Actor-Critic</title>
    <link href="https://xdmqp.github.io/2020/08/05/Actor-Critic/"/>
    <id>https://xdmqp.github.io/2020/08/05/Actor-Critic/</id>
    <published>2020-08-04T16:00:00.000Z</published>
    <updated>2021-07-06T03:09:18.865Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h2><h3 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h3><p>Actor-Critic方法分为<strong>演员Actor和评价者Critic</strong>两部分：Actor使用策略梯度方法中的策略函数，负责生成动作并与环境交互，Critic使用价值函数来评估Actor的表现，并指导Actor的下一阶段工作，也就相当于替换掉了上节策略梯度方法的蒙特卡洛法。</p><p>由于Actor-Critic有两部分，因此算法中需要做两组近似：<strong>策略函数的近似和价值函数的近似</strong>，即：<br>$$<br>策略函数近似：\pi_{\theta}(s,a) = P(a|s,\theta)\approx  \pi(a|s)<br>$$<br>$$<br>状态价值函数近似：\hat{v}(s, w) \approx v_{\pi}(s)<br>$$</p><p>$$<br>动作价值函数近似：\hat{q}(s,a,w) \approx q_{\pi}(s,a)<br>$$</p><p>我们可以通过改进蒙特卡洛梯度reinforce算法实现Actor-Critic算法</p><p>先来看一下蒙特卡洛策略梯度reinforce算法中的策略参数更新公式为：<br>$$<br>\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)  v_t<br>$$<br><strong>其中，$\nabla_{\theta}log \pi_{\theta}(s_t,a_t)$是分值函数，无需改动，需要改动的是$v_t$部分，不再使用蒙特卡洛方法，而是参考之前DQN的做法，使用一个Q网络作为Critic，Q网络的输入可以是状态，输出是每个动作的价值或者最优动作的价值。</strong></p><p>这样Actor就可以利用$v_t$这个最优价值迭代更新策略函数的参数$\theta$，从而选择动作，并得到reward和新状态，Critic使用reward和新状态更新网络参数$w$，接着使用新的网络参数$w$计算最优价值$v_t$。</p><h3 id="Critic可选形式"><a href="#Critic可选形式" class="headerlink" title="Critic可选形式"></a>Critic可选形式</h3><p>Critic评估点的形式主要有：</p><p>状态价值：<br>$$<br>\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t) V(s,w)<br>$$<br>动作价值：<br>$$<br>\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t) Q(s,a,w)<br>$$<br>TD误差：<br>$$<br>\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)\delta(t)<br>$$<br>$$<br>\delta(t) = R_{t+1} + \gamma V(S_{t+1}) -V(S_t)\quad or \quad\delta(t) = R_{t+1} + \gamma Q(S_{t+1}，A_{t+1} ) -Q(S_t,A_t)<br>$$</p><p>Advantage函数：<br>$$<br>\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)A(S,A,w,\beta)<br>$$<br>$$<br>A(S,A,w,\beta) = Q(S,A, w, \alpha, \beta) - V(S,w,\alpha)<br>$$</p><p>Td(λ)误差：<br>$$<br>\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)\delta(t)E_(t)<br>$$<br>Critic本身的模型参数$w$，一般采用均方误差损失函数进行迭代更新，以最简单的线性Q函数：$Q(s,a ,w) = \phi(s,a)^Tw$为例，$w$的更新方式可以表示为：<br>$$<br>\delta = R_{t+1} + \gamma Q(S_{t+1}，A_{t+1} ) -Q(S_t,A_t)<br>$$</p><p>$$<br>w = w+ \beta\delta\phi(s,a)<br>$$</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>Critic评估点基于TD误差，使用神经网络计算TD误差更新网络参数，Actor也使用神经网络更新网络参数</p><p>输入：<strong>迭代轮数$T$,状态特征维度$n$,动作集合$A$,步长$\alpha$,$\beta$,衰减因子$\gamma$,探索率$\epsilon$,Critic网络结构和Actor网络结构</strong><br>输出：<strong>Actor网络参数$\theta$,Critic网络参数$w$</strong></p><ol><li><p>随机初始化所有状态和动作对应的价值$Q$</p></li><li><p>for i from i to T:<br>初始化S为当前状态序列的第一个状态，拿到其特征向量$\phi(S)$<br>在Actor网络中将$\phi(S)$作为输入，输出动作A，基于动作A得到新状态$S’$，反馈$R$<br>在Critic网络中分别使用$\phi(S)， \phi(S‘’)$作为输入，得到Q值输出$V(S)， V(S’)$<br>计算TD误差$\delta = R +\gamma V(S’) -V(S)$</p><p>使用均方误差损失函数$\sum\limits(R +\gamma V(S’) -V(S,w))^2$进行Critic网络参数$w$的更新<br>更新Actor网络参数$\theta$:<br>$\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(S_t,A)\delta$</p></li></ol><p>Actor的分值函数$\nabla_{\theta}log \pi_{\theta}(S_t,A)$可以选择<strong>softmax或者高斯分值函数</strong></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>基础版Actor-Critic算法思路很好，但是难以收敛，因此仍需进行改进，接下来我们将要讨论Actor-Critic的改进方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Actor-Critic&quot;&gt;&lt;a href=&quot;#Actor-Critic&quot; class=&quot;headerlink&quot; title=&quot;Actor-Critic&quot;&gt;&lt;/a&gt;Actor-Critic&lt;/h2&gt;&lt;h3 id=&quot;算法简介&quot;&gt;&lt;a href=&quot;#算法简介&quot; cla
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://xdmqp.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy-Based RL" scheme="https://xdmqp.github.io/tags/Policy-Based-RL/"/>
    
      <category term="Actor-Critic" scheme="https://xdmqp.github.io/tags/Actor-Critic/"/>
    
  </entry>
  
  <entry>
    <title>策略梯度(Policy Gradient)方法</title>
    <link href="https://xdmqp.github.io/2020/08/04/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/"/>
    <id>https://xdmqp.github.io/2020/08/04/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/</id>
    <published>2020-08-03T16:00:00.000Z</published>
    <updated>2021-07-06T08:22:46.293Z</updated>
    
    <content type="html"><![CDATA[<h2 id="策略梯度-Policy-Gradient-方法"><a href="#策略梯度-Policy-Gradient-方法" class="headerlink" title="策略梯度(Policy Gradient)方法"></a>策略梯度(Policy Gradient)方法</h2><h3 id="Value-Based-RL的不足"><a href="#Value-Based-RL的不足" class="headerlink" title="Value-Based RL的不足"></a>Value-Based RL的不足</h3><ul><li>对连续动作的处理能力不足。DQN相关方法只能处理离散动作，无法处理连续动作，虽然也有学者提出了连续控制DQN方法—<a href="https://arxiv.org/abs/1603.00748" target="_blank" rel="noopener">NAF</a>，但是实现过程比较复杂。</li><li>对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态在建模后拥有的特征描述，很有可能导致Value-Based RL无法得到最优解，而Policy-Based RL则可以有效解决这个问题</li><li>无法解决随机策略问题。Value-Based RL对应的最优策略通常是确定性策略，即从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略为随机策略，这种情况下就需要使用Policy-Based RL方法</li></ul><h3 id="Policy-Based-RL的引入"><a href="#Policy-Based-RL的引入" class="headerlink" title="Policy-Based RL的引入"></a>Policy-Based RL的引入</h3><p>在Value-Based RL中，采用$\hat{q}$将动作价值函数近似表示为一个包含参数$w$的函数，接受状态和动作的输入，计算得到近似的动作价值，即：<br>$$<br>\hat{q}(s,a,w) \approx q_{\pi}(s,a)<br>$$<br>在Policy-Based RL中，采用类似思路将策略近似表示为一个包含参数$\theta$的函数，即：<br>$$<br>\pi_{\theta}(s,a) = P(a|s,\theta)\approx  \pi(a|s)<br>$$<br>将策略表示为一个连续函数后，就可以用连续函数的优化方法来寻找最优策略了，最常用的方法就是<strong>梯度上升法</strong></p><h3 id="策略梯度的优化目标"><a href="#策略梯度的优化目标" class="headerlink" title="策略梯度的优化目标"></a>策略梯度的优化目标</h3><p>用梯度上升寻找最优梯度，首先要找到一个可以优化的函数目标</p><p>最简单的优化目标就是初始状态收获的期望值，即：<br>$$<br>J_1(\theta) = V_{\pi_{\theta}}(s_1) = \mathbb{E}<em>{\pi</em>{\theta}}(G_1)<br>$$<br>但有的问题是没有明确的初始状态的，那么优化目标就可以定义为平均价值，即：<br>$$<br>J_{avV}(\theta) =\sum\limits_sd_{\pi_{\theta}}(s)V_{\pi_{\theta}}(s)<br>$$<br>其中，$d_{\pi_{\theta}}(s)$是基于策略$\pi_{\theta}$生成的马尔科夫链关于状态的静态分布</p><p>或者定义为每一时间步的平均奖励，即：<br>$$<br>J_{avR}(\theta) = =\sum\limits_sd_{\pi_{\theta}}(s) \sum\limits_a \pi_{\theta}(s,a) R_s^a<br>$$<br>无论是采用$J_1,J_{avV}$还是$J_{avR}$来优化目标，最终对$\theta$求导的梯度都可以表示为：<br>$$<br>\nabla_{\theta} J(\theta) = \mathbb{E}<em>{\pi</em>{\theta}}[\nabla_{\theta}log \pi_{\theta}(s,a) Q_{\pi}(s,a)]<br>$$<br>论文:<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf" target="_blank" rel="noopener">https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf</a></p><p>我们也可以采用其他可能的优化目标来做梯度上升，此时梯度式子里面的$\nabla_{\theta}log \pi_{\theta}(s,a)$并不改变，变化的只是后面的$Q_{\pi}(s,a)]$部分。$\nabla_{\theta}log \pi_{\theta}(s,a)$称为分值函数(score function)</p><p>接下来就是策略函数$\pi_{\theta}(s,a)$的设计了</p><h3 id="策略函数设计"><a href="#策略函数设计" class="headerlink" title="策略函数设计"></a>策略函数设计</h3><p>最常用的策略函数就是<strong>softmax策略函数</strong>，它主要应用于离散空间中，softmax策略使用描述状态和行为的特征$\phi(s,a)$与参数的线性组合来权衡一个行为发生的几率，即：<br>$$<br>\pi_{\theta}(s,a) = \frac{e^{\phi(s,a)^T\theta}}{\sum\limits_be^{\phi(s,b)^T\theta}}<br>$$<br>求导即得对应的分值函数：<br>$$<br>\nabla_{\theta}log \pi_{\theta}(s,a) = \phi(s,a) - \mathbb{E}<em>{\pi</em>{\theta}}[\phi(s,.)]<br>$$<br>另一种策略函数是<strong>高斯策略</strong>，它主要应用于连续行为空间中，该策略对应的行为从高斯分布$\mathbb{N(\phi(s)^T\theta, \sigma^2)}$中产生，高斯策略函数求导即得对应的分值函数：<br>$$<br>\nabla_{\theta}log \pi_{\theta}(s,a) =   \frac{(a-\phi(s)^T\theta)\phi(s)}{\sigma^2}<br>$$<br>有了策略梯度和策略函数，就能得到最简单的策略梯度算法了。</p><h3 id="蒙特卡洛策略梯度reinforce算法"><a href="#蒙特卡洛策略梯度reinforce算法" class="headerlink" title="蒙特卡洛策略梯度reinforce算法"></a>蒙特卡洛策略梯度reinforce算法</h3><p>使用价值函数$v(s)$近似策略梯度中的$Q_{\pi}(s,a)$的流程如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">输入：N个蒙特卡洛完整序列，训练步长</span><br><span class="line">输出：策略函数的参数</span><br><span class="line">1. for 每个蒙特卡洛序列：</span><br><span class="line">a. 用蒙特卡罗法计算序列每个时间位置t的状态价值vt</span><br><span class="line">b. 对序列每个时间位置t，使用梯度上升法更新策略函数的参数</span><br></pre></td></tr></table></figure><p>$$<br>\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)  v_t<br>$$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2. 返回策略函数参数，这里的策略函数可以是softmax策略，高斯策略或其他策略</span><br></pre></td></tr></table></figure><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>策略梯度提供了与DQN之类的Value-Based RL方法不同的新思路，但是仍然有很多不足。由于采用蒙特卡洛法，因此<strong>需要收集到一个完整的行为序列，将梯度收集到一起再进行算法迭代，并且蒙特卡洛法使用收获的期望来计算状态价值，会导致行为有较多变异性，参数更新的方向很可能不是策略梯度的最优方向</strong>。</p><p>因此，Policy-Based RL方法仍需要改进，随之便产生了将Policy-Based和Value-Based结合的方法：Actor-Critic。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;策略梯度-Policy-Gradient-方法&quot;&gt;&lt;a href=&quot;#策略梯度-Policy-Gradient-方法&quot; class=&quot;headerlink&quot; title=&quot;策略梯度(Policy Gradient)方法&quot;&gt;&lt;/a&gt;策略梯度(Policy Gradi
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://xdmqp.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy-Based RL" scheme="https://xdmqp.github.io/tags/Policy-Based-RL/"/>
    
      <category term="策略梯度" scheme="https://xdmqp.github.io/tags/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>Q-Learning和Deep Q-Learning(DQN)</title>
    <link href="https://xdmqp.github.io/2020/07/23/DQN%E5%85%A5%E9%97%A8/"/>
    <id>https://xdmqp.github.io/2020/07/23/DQN%E5%85%A5%E9%97%A8/</id>
    <published>2020-07-23T07:48:38.000Z</published>
    <updated>2021-07-06T08:14:32.956Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p><strong>Q-Learning更新Q值方法：</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://www.zhihu.com/equation?tex=Q%28S_%7Bt%7D%2CA_%7Bt%7D%29+%5Cleftarrow+Q%28S_%7Bt%7D%2CA_%7Bt%7D%29%2B%5Calpha%28%7BR_%7Bt%2B1%7D%2B%5Clambda+%5Cmax+_aQ%28S_%7Bt%2B1%7D%2Ca%29%7D+-+Q%28S_t%2CA_t%29%29" alt="[公式]" title="">                </div>                <div class="image-caption">[公式]</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://pic3.zhimg.com/80/96ee91330ffbcc21b0ac399f41372e45_720w.png" alt="Q-Learning算法" title="">                </div>                <div class="image-caption">Q-Learning算法</div>            </figure><p>Exploration and Exploitation</p><p>Exploration：随机产生action</p><p>Exploitation：根据当前Q值计算最优action，这种策略π成为greedy policy贪婪策略，也就是</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://www.zhihu.com/equation?tex=%5Cpi%28S_%7Bt%2B1%7D%29+%3D+arg%5Cmax+_aQ%28S_%7Bt%2B1%7D%2Ca%29%0A" alt="[公式]" title="">                </div>                <div class="image-caption">[公式]</div>            </figure><p>将两者结合起来就是ε-greedy策略，ε是一个很小的值，作为随机选择动作的概率</p><p><strong>ε-greedy策略是一种极其简单粗暴的方法，对于一些复杂的任务采用这种方法来探索未知空间是不可取的。因此，最近有越来越多的方法来改进这种探索机制</strong></p><p>Q-Learning算法采用表格来储存Q值，但是在现实问题中存在的state太多，用表格是存放不下的，这就需要对状态的维度进行压缩，解决方法就是价值近似函数Value Function Approximation。</p><h3 id="价值近似函数-Value-Function-Approximation"><a href="#价值近似函数-Value-Function-Approximation" class="headerlink" title="价值近似函数(Value Function Approximation)"></a>价值近似函数(Value Function Approximation)</h3><p>用一个带有参数$w$函数来近似表示Q(s,a)，即<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29%5Capprox+f%28s%2Ca%2Cw%29" alt="[公式]" title="">                </div>                <div class="image-caption">[公式]</div>            </figure></p><p>$f$可以是任意类型的函数</p><p>当采用了价值近似函数后，只需将state作为输入，输出的是一个包含所有动作Q值的向量<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://www.zhihu.com/equation?tex=%5BQ%28s%2Ca_1%29%2CQ%28s%2Ca_2%29%2CQ%28s%2Ca_3%29%2C...%2CQ%28s%2Ca_n%29%5D" alt="[公式]" title="">                </div>                <div class="image-caption">[公式]</div>            </figure></p><p>到这我们就看到了Q-Learning与深度学习的结合点：<strong>用深度神经网络来表示价值近似函数$f$，从而产生了Deep Q-Learning</strong>。</p><h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><p>神经网络需要定义一个损失函数Loss Function，也就是标签和网络输出的偏差，目标是让损失函数最小化。为此，我们需要有样本和大量的标签数据，然后通过梯度下降Gradient Descent和反向传播Back Propagation的方法实现神经网络参数的迭代更新。其中的标签数据就是Q-Learning中的目标Q值：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D%2B%5Clambda+%5Cmax+_aQ%28S_%7Bt%2B1%7D%2Ca%29" alt="[公式]" title="">                </div>                <div class="image-caption">[公式]</div>            </figure>，我们的目标就是<strong>让Q值趋近于目标Q值</strong>。</p><p>因此，Q-network训练的损失函数为：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://pic4.zhimg.com/80/3858f07818d129668fc83d48d855bb1f_720w.png" alt="img" title="">                </div>                <div class="image-caption">img</div>            </figure><p>其中，$s’,a’$为下一个状态和动作</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://pic2.zhimg.com/80/c24454f472843ef5caef2733d50aba00_720w.png" alt="NIPS2013 DQN" title="">                </div>                <div class="image-caption">NIPS2013 DQN</div>            </figure><p>上图便是NIPS2013版DQN的算法流程，用到了Experience Replay也就是经验回放的技巧</p><h2 id="Nature-DQN"><a href="#Nature-DQN" class="headerlink" title="Nature DQN"></a>Nature DQN</h2><p>在NIPS2013版DQN中目标Q网络随着Q网络的更新而变化，导致<strong>目标Q值和当前Q值得相关性较大</strong>，因此Nature DQN在NIPS DQN基础上增加目标Q网络用来计算目标Q值，目标Q网络的参数还是来自Q网络，只不过是采用<strong>延迟更新</strong>的方式，在经过一定步数迭代后，再将当前Q网络的参数更新到目标Q网络。Nature DQN的损失函数定义如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://pic4.zhimg.com/80/741e46913effd4d0821ff57b54ff5580_720w.png" alt="img" title="">                </div>                <div class="image-caption">img</div>            </figure><p>公式中计算目标Q值使用的很久之前的参数w-，而不是Q网络中的w。</p><p>Nature DQN的算法流程如下图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://mofanpy.com/static/results/reinforcement-learning/4-1-1.jpg" alt="4-1-1.jpg" title="">                </div>                <div class="image-caption">4-1-1.jpg</div>            </figure><h2 id="DQN的几种改进"><a href="#DQN的几种改进" class="headerlink" title="DQN的几种改进"></a>DQN的几种改进</h2><p>Double DQN：减少因为maxQ值带来的计算偏差，也就是过估计(overestimation)问题。Nature DQN用Q网络选择动作和计算目标Q值，Double DQN用当前Q网络选择动作，用目标Q网络计算目标Q值，也就是说Double DQN改变了Nature DQN的目标Q值计算方式，两者的计算公式对比如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://mofanpy.com/static/results/reinforcement-learning/4-5-1.png" alt="4-5-1.png" title="">                </div>                <div class="image-caption">4-5-1.png</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://mofanpy.com/static/results/reinforcement-learning/4-5-2.png" alt="4-5-2.png" title="">                </div>                <div class="image-caption">4-5-2.png</div>            </figure><p>Prioritized Experience Replay：对经验池中的样本赋予优先级，根据优先级来对经验池中的样本进行采样，优先级越高，采样的概率就越高。优先级采用目标Q值与当前Q值的差值来表示。如果每次采样都对样本根据优先级排序，将会大大延长算法时间，SumTree就是一种改进方法。它是一种二叉树结构，样本的优先级p存放在叶子节点中，每个分支节点都是两个叶子节点的和，所以SumTree的根节点就是所有优先级的和sum(p)。树结构如下图所示，假设优先级p为[3,10,12,4,1,2,8,2]，则sum(p)为42。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://mofanpy.com/static/results-small/reinforcement-learning/4-6-2.png" alt="SumTree结构" title="">                </div>                <div class="image-caption">SumTree结构</div>            </figure><p>抽样时，用优先级的总和sum(p)除以采样数batch size，将[0,sum]区间分割成n(n=batch size)个子区间，比如上图就可以分割为[0-7],[7-14],[14-21],[21-28],[28-35],[35-42]，然后在每个子区间里随机选取一个数，比如在[14-21]中选择了17，然后从SumTree的根节点向下开始搜索，先对比左孩子节点的值，因为17&lt;42，就从左子树接着搜索，否则从右子树搜索（减去左孩子节点值），接着17&gt;13，就将17-13=4，再将4和16的左孩12对比，因为4&lt;12，就将12作为当前区间选择的优先级p，并且取到12对应的样本。</p><p>Dueling DQN：将传统的Q网络分成两个通道，一个输出状态state的Value，一个输出动作action的Advantage，最后再合起来得到Q，如下图所示</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdnimg.cn/2019051710344679.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzMxNjA4Mg==,size_16,color_FFFFFF,t_70" alt="img" title="">                </div>                <div class="image-caption">img</div>            </figure><blockquote><p>参考资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Q-Learning&quot;&gt;&lt;a href=&quot;#Q-Learning&quot; class=&quot;headerlink&quot; title=&quot;Q-Learning&quot;&gt;&lt;/a&gt;Q-Learning&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Q-Learning更新Q值方法：&lt;/strong&gt;&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://xdmqp.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Q-Learning" scheme="https://xdmqp.github.io/tags/Q-Learning/"/>
    
      <category term="DQN" scheme="https://xdmqp.github.io/tags/DQN/"/>
    
  </entry>
  
  <entry>
    <title>Hexo使用问题汇总</title>
    <link href="https://xdmqp.github.io/2020/07/21/Hexo%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/"/>
    <id>https://xdmqp.github.io/2020/07/21/Hexo%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</id>
    <published>2020-07-21T03:32:49.000Z</published>
    <updated>2021-07-06T08:26:07.584Z</updated>
    
    <content type="html"><![CDATA[<h4 id="数学公式渲染问题"><a href="#数学公式渲染问题" class="headerlink" title="数学公式渲染问题"></a>数学公式渲染问题</h4><p>问题描述：当md文件中包含一些比较复杂的数学公式时，在使用Typora编辑时可以正常显示数学公式，但是使用hexo g命令将md文件解析成html文件时，会报如下错误：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/hexo_error.png" alt="hexo_error" title="">                </div>                <div class="image-caption">hexo_error</div>            </figure><p>原因：当Latex公式中出现连续两个及以上左大括号”{“时，就会报上述错误</p><p>解决方法：每当出现连续两个及以上左大括号”{“时，使用空格将其隔开，就能正常解析了</p><h4 id="行内公式"><a href="#行内公式" class="headerlink" title="行内公式"></a>行内公式</h4><p>问题描述：Typora编辑器给出的格式只有公式块，不能定义行内公式</p><p>解决方式：在需要输入行内公式的地方前后各用一个$包围起来</p><p>如$s_t$就可以像下面这样输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$s_t$</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;数学公式渲染问题&quot;&gt;&lt;a href=&quot;#数学公式渲染问题&quot; class=&quot;headerlink&quot; title=&quot;数学公式渲染问题&quot;&gt;&lt;/a&gt;数学公式渲染问题&lt;/h4&gt;&lt;p&gt;问题描述：当md文件中包含一些比较复杂的数学公式时，在使用Typora编辑时可以正常显示数学
      
    
    </summary>
    
    
    
      <category term="Hexo渲染公式" scheme="https://xdmqp.github.io/tags/Hexo%E6%B8%B2%E6%9F%93%E5%85%AC%E5%BC%8F/"/>
    
      <category term="行内公式" scheme="https://xdmqp.github.io/tags/%E8%A1%8C%E5%86%85%E5%85%AC%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>神经网络反向传播过程</title>
    <link href="https://xdmqp.github.io/2020/07/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B/"/>
    <id>https://xdmqp.github.io/2020/07/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B/</id>
    <published>2020-07-19T02:08:25.000Z</published>
    <updated>2021-07-06T08:23:35.161Z</updated>
    
    <content type="html"><![CDATA[<h3 id=""><a href="#" class="headerlink" title=""></a><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630141449671-1058672778.png" alt="img" style="zoom:50%;" /></h3><p>以图中的神经网络为例，第一层为输入层，包含了两个神经元i1，i2和截距项b1，第二层是隐藏层，包含两个神经元h1,h2和截距项b2；第三层是输出层o1,o2，激活函数采用sigmoid函数。</p><p><strong>上述神经网络中的训练过程如下：</strong></p><p><strong>Step1 前向传播</strong></p><ol><li>计算神经元h1、h2的输入加权和</li></ol><p>$$<br>ne{t_{h1}} = w1\times i1 + w2\times i2 + b1<br>$$</p><p>$$<br>ne{t_{h2}} = w3 \times i1 + w4 \times i2 + b1<br>$$</p><ol start="2"><li>通过激活函数计算神经元h1、h2的输出o1、o2</li></ol><p>$$<br>Sigmoid函数：S(x)=\frac{1}{1+e^{-x}}<br>$$</p><p>$$<br>ou{t_{h1}} = \frac{1}{ {1 + {e^{ - net_{h1}}}}}<br>$$</p><p>$$<br>ou{t_{h2}} = \frac{1}{ {1 + {e^{ - net_{h2}}}}}<br>$$</p><ol start="3"><li><p>计算输出层o1、o2的输入加权和</p><p>与步骤1同理</p></li><li><p>通过激活函数计算输出层o1、o2的输出</p><p>与步骤2同理</p></li></ol><p><strong>Setp2 反向传播</strong></p><ol><li>计算均方误差，计算公式为</li></ol><p>$$<br>{E_{total}} = \sum {\frac{1}{n}} {(t\arg et - ouput)^2}<br>$$</p><ol start="2"><li><p>隐含层到输出层的权值更新</p><p>以权值w5为例，通过对误差求w5的偏导确定w5对整体误差产生了多大的影响，并应用链式法则进行化简</p></li></ol><p>$$<br>\frac{ {\partial {E_{total}}}}{ {\partial w5}} = \frac{ {\partial {E_{total}}}}{ {\partial ou{t_{o1}}}}\times\frac{ {\partial ou{t_{o1}}}}{ {\partial ne{t_{o1}}}}\times\frac{ {\partial ne{t_{o1}}}}{ {\partial w5}}<br>$$</p><p>   其中，<br>$$<br>\frac{ {\partial {E_{total}}}}{ {\partial ou{t_{o1}}}} = \frac{ {\partial (\frac{1}{2}\times[{ {(t\arg e{t_{o1}} - ou{t_{o1}})}^2} + { {(t\arg e{t_{o2}} - ou{t_{o2}})}^2}])}}{ {\partial ou{t_{o1}}}} =  - (t\arg e{t_{o1}} - ou{t_{o1}})<br>$$</p><p>$$<br>\frac{ {\partial ou{t_{o1}}}}{ {\partial ne{t_{o1}}}} = \frac{ {\partial \frac{1}{ {1 + {e^{ - ne{t_{o1}}}}}}}}{ {\partial ne{t_{o1}}}} = ou{t_{o1}}\times(1 - ou{t_{o1}})<br>$$</p><p>$$<br>\frac{ {\partial ne{t_{o1}}}}{ {\partial w5}} = \frac{ {\partial (w5 \times ou{t_{h1}} + w6 \times ou{t_{h2}} + {b_2})}}{ {\partial w5}} = ou{t_{h1}}<br>$$</p><p>   对w5的值进行更新</p><p>$$<br>w{5^+} = w5 - \eta \times \frac{ {\partial {E_{total}}}}{ {\partial w5}}<br>$$</p><p>   同理，对w6,w7,w8进行更新</p><ol start="3"><li><p>输入层到隐含层的权值更新</p><p>与步骤2同理</p></li></ol><blockquote><p>参考资料：</p><p>从此蜕变：<a href="https://www.cnblogs.com/codehome/p/9718611.html" target="_blank" rel="noopener">https://www.cnblogs.com/codehome/p/9718611.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;&quot;&gt;&lt;/a&gt;&lt;img src=&quot;https://images2015.cnblogs.com/blog/853467/201606/853467-20160630141449671-1
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="https://xdmqp.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="反向传播" scheme="https://xdmqp.github.io/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
  </entry>
  
  <entry>
    <title>特征归一化方法</title>
    <link href="https://xdmqp.github.io/2020/07/15/%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    <id>https://xdmqp.github.io/2020/07/15/%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96%E6%96%B9%E6%B3%95/</id>
    <published>2020-07-14T16:00:00.000Z</published>
    <updated>2021-07-06T08:23:42.441Z</updated>
    
    <content type="html"><![CDATA[<h1 id="特征归一化方法"><a href="#特征归一化方法" class="headerlink" title="特征归一化方法"></a>特征归一化方法</h1><h2 id="线性函数归一化（Min-Max-scaling），归一化公式如下："><a href="#线性函数归一化（Min-Max-scaling），归一化公式如下：" class="headerlink" title="线性函数归一化（Min-Max scaling），归一化公式如下："></a>线性函数归一化（Min-Max scaling），归一化公式如下：</h2><p>$$<br>X_{\text {norm}}=\frac{X-X_{\text {min}}}{X_{\text {max}}-X_{\text {min}}}<br>$$</p><p>X为原始数据，Xmin和Xmax分别为原始数据的最小值和最大值</p><h2 id="0均值归一化（Z-score-standardization），归一化公式如下："><a href="#0均值归一化（Z-score-standardization），归一化公式如下：" class="headerlink" title="0均值归一化（Z-score standardization），归一化公式如下："></a>0均值归一化（Z-score standardization），归一化公式如下：</h2><p>$$<br>z=\frac{x-μ}{θ}<br>$$</p><p>x为原始数据，μ和θ分别为原始数据集的均值和方差，<strong>这种方法适用近似为高斯分布的原始数据</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;特征归一化方法&quot;&gt;&lt;a href=&quot;#特征归一化方法&quot; class=&quot;headerlink&quot; title=&quot;特征归一化方法&quot;&gt;&lt;/a&gt;特征归一化方法&lt;/h1&gt;&lt;h2 id=&quot;线性函数归一化（Min-Max-scaling），归一化公式如下：&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征归一化" scheme="https://xdmqp.github.io/tags/%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用命令</title>
    <link href="https://xdmqp.github.io/2020/07/12/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>https://xdmqp.github.io/2020/07/12/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</id>
    <published>2020-07-11T16:00:00.000Z</published>
    <updated>2021-07-06T02:52:35.714Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Linux常用命令"><a href="#Linux常用命令" class="headerlink" title="Linux常用命令"></a>Linux常用命令</h3><h4 id="ls命令"><a href="#ls命令" class="headerlink" title="ls命令"></a><strong>ls命令</strong></h4><p><strong>用法：ls [option]，列出当前目录下的文件</strong></p><p>  -l：列出文件详细信息</p><p>  -a :列出目录所有文件，包含以.开始的隐藏文件</p><p>  -t：以时间排序，默认最新修改的文件在上面</p><p> -r：倒序输出</p><p> -x：按列输出，横向排序</p><p> -u：按照文件上次被访问的时间排序</p><p> -S：以文件大小排序</p><p> -h：以易读大小显示</p><p> <strong>组合命令，如-lrt，代表了”-l -r -t”，按修改时间倒序列出当前目录下的文件</strong></p><h4 id="grep"><a href="#grep" class="headerlink" title="grep"></a><strong>grep</strong></h4><p><strong>用法：grep [option] pattern files，正则表达式搜索文本</strong></p><p>-r：搜索子目录</p><p>-n：显示匹配行和行号</p><p>-l：只列出匹配的文件名</p><p>-i：不区分大小写，默认区分大小写</p><p>-w：只匹配整个单词，而不是字符串的一部分</p><p>grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的行</p><p>grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行</p><h4 id="vim-vi：文件编辑"><a href="#vim-vi：文件编辑" class="headerlink" title="vim/vi：文件编辑"></a><strong>vim/vi：文件编辑</strong></h4><p>vi/vim 共分为三种模式，分别是<strong>命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）</strong>， 三种模式的切换方式如下图所示</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://www.runoob.com/wp-content/uploads/2014/07/vim-vi-workmodel.png" alt="**vim/vi工作模式切换**" title="">                </div>                <div class="image-caption">**vim/vi工作模式切换**</div>            </figure><h5 id="命令模式下常用命令"><a href="#命令模式下常用命令" class="headerlink" title="命令模式下常用命令"></a>命令模式下常用命令</h5><ol><li>i:进入输入模式，从光标当前位置开始输入</li><li>a:进入输入模式，从光标所在位置的下一个位置开始输入</li><li>o：进入输入模式，插入新的一行，从行首开始输入</li><li><strong>0:移动光标到文件开头</strong></li><li><strong>G：移动光标到文件最后</strong></li><li>$：移动光标到所在行行尾</li><li>^：移动光标到所在行行首</li><li>#l：移动光标到当前行的第#个位置</li><li>x:删除光标所在位置后面的一个字符</li><li>X：删除光标所在位置前面的一个字符</li><li><strong>dd：删除光标所在行</strong></li><li>#dd：从光标所在行开始删除#行</li><li><strong>ctrl+g：列出光标所在行行号</strong></li><li><strong>#G：移动到光标只文件的第#行行首</strong></li><li>u：撤销操作</li><li>ctrl+r：恢复操作</li></ol><h5 id="底线命令模式下常用命令"><a href="#底线命令模式下常用命令" class="headerlink" title="底线命令模式下常用命令"></a>底线命令模式下常用命令</h5><ol><li><strong>set nu：显示文件行号</strong></li><li>set nonu：隐藏文件行号</li><li><strong>#：跳到文件中的第#行</strong></li><li><strong>/关键字：查找指定关键字，按n会继续往后查找</strong></li><li>w：保存文件</li><li><strong>q：退出文件编辑</strong></li><li><strong>wq：保存并退出</strong></li><li><strong>q!：放弃一切更改，强制退出</strong></li></ol><h4 id="cd命令"><a href="#cd命令" class="headerlink" title="cd命令"></a><strong>cd命令</strong></h4><p><strong>用法 cd [option] path</strong></p><p>/path:切换到根目录中的path目录下</p><p>./path：切换到当前目录的path目录下</p><p>../path:切换到上层目录的path目录下</p><p>~：切换到当前用户的自家目录下，如果为root用户，就进去root目录下</p><p>-：切换到进入当前目录的前的目录下</p><h4 id="pwd命令"><a href="#pwd命令" class="headerlink" title="pwd命令"></a>pwd命令</h4><p><strong>打印当前工作目录路径</strong></p><p>-L：打印逻辑工作目录路径</p><p>-P：打印物理工作目录路径</p><h4 id="mkdir命令"><a href="#mkdir命令" class="headerlink" title="mkdir命令"></a>mkdir命令</h4><p><strong>用法：mkdir [option] dirname，创建文件夹</strong></p><p>-m：对创建文件夹设置存取权限</p><p>-p：在指定目录下创建文件夹 </p><h4 id="rm命令"><a href="#rm命令" class="headerlink" title="rm命令"></a>rm命令</h4><p><strong>用法：rm [option] name，删除文件或者目录</strong></p><p>-i ：删除前逐一确认</p><p>-f：直接删除，无需确认</p><p>-r：删除指定目录及以下的所有文件</p><h4 id="rmdir命令"><a href="#rmdir命令" class="headerlink" title="rmdir命令"></a>rmdir命令</h4><p><strong>用法： rm [-p] name，删除空目录</strong></p><h4 id="rz命令"><a href="#rz命令" class="headerlink" title="rz命令"></a>rz命令</h4><p><strong>用法：rz [option]，使用ZMODEM协议将本地文件上传到远程Linux服务器，不能操作文件夹</strong></p><p>-+, –append:将文件内容追加到已存在的同名文件</p><p>-a,–ascii:以文本方式传输</p><p>-b, –binary:以二进制方式传输，推荐使用</p><p>–delay-startup N:等待N秒</p><p>-e, –escape:对所有控制字符转义，建议使用</p><p>-E, –rename:已存在同名文件则重命名新上传的文件，以点和数字作为后缀</p><p>-p, –protect:对ZMODEM协议有效，如果目标文件已存在则跳过 -</p><p>q, –quiet:安静执行，不输出提示信息</p><p>-v, –verbose:输出传输过程中的提示信息</p><p>-y, –overwrite:存在同名文件则替换</p><p>-X, –xmodem:使用XMODEM协议</p><p>–ymodem:使用YMODEM协议</p><p>-Z, –zmodem:使用ZMODEM协议</p><p>–version：显示版本信息</p><p>–h, –help：显示帮助信息</p><h4 id="sz命令"><a href="#sz命令" class="headerlink" title="sz命令"></a>sz命令</h4><p><strong>用法：sz [option] [filelist]，使用ZMODEM协议将多个文件从远程服务器下载到本地，不能操作文件夹</strong></p><p><strong>示例：sz file1 file2 file3</strong></p><p>选项和rz相同</p><h4 id="压缩命令"><a href="#压缩命令" class="headerlink" title="压缩命令"></a>压缩命令</h4><blockquote><p><a href="https://www.cnblogs.com/dch0/p/11111626.html" target="_blank" rel="noopener">https://www.cnblogs.com/dch0/p/11111626.html</a></p></blockquote><h4 id="mv命令"><a href="#mv命令" class="headerlink" title="mv命令"></a>mv命令</h4><p><strong>用法：mv [option]</strong></p><table><thead><tr><th>命令</th><th>操作</th></tr></thead><tbody><tr><td>mv 文件名 文件名</td><td>将源文件名改为目标文件名</td></tr><tr><td>mv 文件名 目录名</td><td>将文件移动到目标目录</td></tr><tr><td>mv 目录名 目录名</td><td>目标目录已存在，将源目录移动到目标目录；目标目录不存在则改名</td></tr></tbody></table><p>-i: 若指定目录已有同名文件，则先询问是否覆盖旧文件</p><p>-f: 在 mv 操作要覆盖某已有的目标文件时不给任何指示</p><h4 id="sed命令"><a href="#sed命令" class="headerlink" title="sed命令"></a>sed命令</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed [-hnV][-e&lt;script&gt;][-f&lt;script文件&gt;][文本文件]</span><br></pre></td></tr></table></figure><p>参数说明：</p><p>-e<script>或--expression=<script> 以选项中指定的script来处理输入的文本文件。</p><p>-f<script文件>或--file=<script文件> 以选项中指定的script文件来处理输入的文本文件。</p><p>脚本说明：</p><ul><li>a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～</li><li>c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！</li><li>d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚；</li><li>i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；</li><li>p ：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～</li><li>s ：取代，可以直接进行取代的工作，通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Linux常用命令&quot;&gt;&lt;a href=&quot;#Linux常用命令&quot; class=&quot;headerlink&quot; title=&quot;Linux常用命令&quot;&gt;&lt;/a&gt;Linux常用命令&lt;/h3&gt;&lt;h4 id=&quot;ls命令&quot;&gt;&lt;a href=&quot;#ls命令&quot; class=&quot;headerli
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://xdmqp.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://xdmqp.github.io/tags/Linux/"/>
    
      <category term="Linux命令" scheme="https://xdmqp.github.io/tags/Linux%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>概念和方法</title>
    <link href="https://xdmqp.github.io/2020/07/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5%E5%92%8C%E6%96%B9%E6%B3%95/"/>
    <id>https://xdmqp.github.io/2020/07/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5%E5%92%8C%E6%96%B9%E6%B3%95/</id>
    <published>2020-07-08T00:37:18.000Z</published>
    <updated>2021-07-06T03:19:33.627Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概念和方法"><a href="#概念和方法" class="headerlink" title="概念和方法"></a>概念和方法</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1. 概念"></a>1. 概念</h3><p>强化学习是机器学习中的一类，类似于无监督学习，强化学习是在没有样本的情况下，通过个体不断与环境(environment)进行交互，从环境的反馈(reward)中不断积累经验从而获取最佳策略(policy)，在环境中达到自身的目的，这是一种试错性的学习，强化学习的基本模型如下图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/rl.png" alt="强化学习基本模型" title="">                </div>                <div class="image-caption">强化学习基本模型</div>            </figure><p><strong>强化学习要素：</strong></p><p>环境状态$state$，$S_t$代表t时刻环境所处的状态</p><p>个体动作$action$，$A_t$代表t时刻个体采取的动作</p><p>环境奖励$reward$，$R_t$代表t-1时刻(上一时刻)个体采取动作$A_{t - 1}$得到的奖励</p><p>个体策略π:个体选择action的依据</p><p>价值函数$v_π(s)$：个体在策略π指导下选择action后的价值，价值函数的表达式如下：<br>$$<br>{v_\pi }(s) = { {\rm{E}}<em>\pi }({R</em>{t + 1}} + \gamma {R_{t + 2}} + {\gamma ^2}{R_{t + 3}} +  \cdots |{S_t} = s)<br>$$<br>奖励衰减因子γ：γ是一个位于[0,1]之间的值，当γ为0时代表价值函数只由当前state产生的reward决定，当γ为1时代表当前state产生的reward和后续state产生的reward对价值函数的影响是等价的</p><p>环境状态转化模型$P_{ss’}^a$：代表在状态s下采取动作a转到状态s’的概率</p><p>贪婪策略$\epsilon$：$\epsilon$也是一个位于[0,1]之间的值，代表了在当前状态下随机选择动作的概率，当$\epsilon$为0时代表在当前状态下只会选择价值函数最高的动作，不会随机选择动作</p><h3 id="2-强化学习方法分类"><a href="#2-强化学习方法分类" class="headerlink" title="2. 强化学习方法分类"></a>2. 强化学习方法分类</h3><ul><li>根据是否需要理解所处的环境可以将方法分为：<strong>Model-Free RL（Q Learning、Sarsa、Policy Gradients等）和Model-Based RL</strong>。Model-Based RL在Model-Free基础上增加了环境建模，并且能够通过想象来判断接下来的所有情况，选择其中最好的一种情况，并依据这种情况才去下一步策略</li><li>根据选择动作时的参考依据可以将方法分为：<strong>基于概率方法（Policy Gradients等）和基于价值方法（Q Learning、Sarsa等）</strong>。基于概率方法是通过分析所处的环境，直接输出下一步要采用的各种动作的概率，然后根据概率采用某一个动作，即每个动作都可能被选中，只是概率不同。基于价值方法是直接输出所有动作的价值，然后根据最高价值来选择动作。<strong>通过结合两种方法的优势，产生一种新方法：Actor-Critic，actor基于概率做出动作，critic对做出的动作给出动作价值。</strong></li><li>根据更新方式可以将方法分为：<strong>回合更新方法（Mote-carlo learning、基础 Policy Gradients等）和单步更新方法（Q Learning、Sarsa和升级Policy Gradients）</strong>。回合更新方法是在执行完一个行为序列后，将梯度收集到一起，再进行更新。单步更新方法是指每一次和环境交互后，立即更新参数。</li><li>根据交互过程中数据的使用方式可以将方法分为：<strong>在线学习方法（Sarsa、Sarsa Lambda）和离线学习方法（Q Learning）</strong>。在线学习方法是指在完成一次交互后，立即用本次交互得到的经验更新行为准则。离线学习方法是先将n次交互的经验存储起来，而后再从存储的经验中更新行为准则。</li></ul><blockquote><p>参考资料：</p><p>强化学习（一）模型基础：<a href="https://www.cnblogs.com/pinard/p/9385570.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/9385570.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概念和方法&quot;&gt;&lt;a href=&quot;#概念和方法&quot; class=&quot;headerlink&quot; title=&quot;概念和方法&quot;&gt;&lt;/a&gt;概念和方法&lt;/h2&gt;&lt;h3 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;headerlink&quot; title=&quot;1. 概念
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xdmqp.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://xdmqp.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概念" scheme="https://xdmqp.github.io/tags/%E6%A6%82%E5%BF%B5/"/>
    
      <category term="方法" scheme="https://xdmqp.github.io/tags/%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Markdown基本语法</title>
    <link href="https://xdmqp.github.io/2020/07/07/Markdown/"/>
    <id>https://xdmqp.github.io/2020/07/07/Markdown/</id>
    <published>2020-07-07T00:37:18.000Z</published>
    <updated>2021-07-06T08:21:57.974Z</updated>
    
    <content type="html"><![CDATA[<h1 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h1><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br><span class="line">### 三级标题</span><br><span class="line">#### 四级标题</span><br><span class="line">##### 五级标题</span><br><span class="line">###### 六级标题</span><br></pre></td></tr></table></figure><p>效果如下：</p><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><hr><h1 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h1><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">**加粗**</span><br><span class="line">*斜体*</span><br><span class="line">***斜体加粗***</span><br><span class="line">&lt;u&gt;下划线&lt;&#x2F;u&gt;</span><br><span class="line">~~删除线~~</span><br></pre></td></tr></table></figure><p>效果如下：<br><strong>加粗</strong><br><em>斜体</em><br><strong><em>斜体加粗</em></strong><br><u>下划线</u><br><del>删除线</del>  </p><hr><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;这就是引用内容</span><br></pre></td></tr></table></figure><p>效果如下:</p><blockquote><p>这就是引用内容</p></blockquote><hr><h1 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h1><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">****</span><br><span class="line">上面是分割线</span><br></pre></td></tr></table></figure><p>效果如下:  </p><hr><p>上面是分割线</p><hr><h1 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h1><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">* 符号列表1</span><br><span class="line">* 符号列表2</span><br><span class="line">* 符号列表3</span><br><span class="line"></span><br><span class="line">1. 数字列表1</span><br><span class="line">2. 数字列表2</span><br><span class="line">3. 数字列表3</span><br></pre></td></tr></table></figure><p>效果如下:</p><ul><li>符号列表1</li><li>符号列表2</li><li>符号列表3</li></ul><ol><li>数字列表1  </li><li>数字列表2  </li><li>数字列表3  </li></ol><hr><h1 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h1><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[我的博客](https:&#x2F;&#x2F;xdmqp.github.io)</span><br></pre></td></tr></table></figure><p>效果如下：<br><a href="https://xdmqp.github.io">我的博客</a></p><hr><h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h1><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![image](&#x2F;img&#x2F;avatar.jpg)</span><br></pre></td></tr></table></figure><p>效果如下:</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/avatar.jpg" alt="image" title="">                </div>                <div class="image-caption">image</div>            </figure><hr><h1 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h1><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">| 姓名 | 性别 | 年龄 |</span><br><span class="line">|  --- |:--:|:--:|</span><br><span class="line">| 李明 | 男 | 23 |</span><br><span class="line">| 李华 | 男 | 20 |</span><br></pre></td></tr></table></figure><p>效果如下：</p><table><thead><tr><th>姓名</th><th align="center">性别</th><th align="center">年龄</th></tr></thead><tbody><tr><td>李明</td><td align="center">男</td><td align="center">23</td></tr><tr><td>李华</td><td align="center">男</td><td align="center">20</td></tr></tbody></table><p><strong>PS：表格与前文要空两行才能正常显示</strong></p><hr><h1 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h1><p>示例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">x&#x3D;a+b</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>效果如下：</p><p>$$<br>x=a+b<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;标题&quot;&gt;&lt;a href=&quot;#标题&quot; class=&quot;headerlink&quot; title=&quot;标题&quot;&gt;&lt;/a&gt;标题&lt;/h1&gt;&lt;p&gt;示例：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;
      
    
    </summary>
    
    
      <category term="Markdown" scheme="https://xdmqp.github.io/categories/Markdown/"/>
    
    
      <category term="Markdown" scheme="https://xdmqp.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper基础</title>
    <link href="https://xdmqp.github.io/2020/07/06/Zookeeper%E5%9F%BA%E7%A1%80/"/>
    <id>https://xdmqp.github.io/2020/07/06/Zookeeper%E5%9F%BA%E7%A1%80/</id>
    <published>2020-07-05T16:00:00.000Z</published>
    <updated>2021-07-06T08:37:46.160Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ZooKeeper基础"><a href="#ZooKeeper基础" class="headerlink" title="ZooKeeper基础"></a>ZooKeeper基础</h1><h2 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201116160146424.png" alt="image-20201116160146424" title="">                </div>                <div class="image-caption">image-20201116160146424</div>            </figure><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201116160219812.png" alt="image-20201116160219812" title="">                </div>                <div class="image-caption">image-20201116160219812</div>            </figure><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><p><strong><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201116160256917.png" alt="image-20201116160256917" title="">                </div>                <div class="image-caption">image-20201116160256917</div>            </figure></strong></p><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>统一命名服务：对应用/服务进行统一命名，便于识别<br>统一配置管理：将配置信息写入一个Znode，各个客户端注册监听这个Znode，一旦Znode数据被修改，ZooKeeper将通知各个客户端服务器<br>统一集群管理：将节点信息写入一个Znode，通过监听Znode获取节点的实时状态变化<br>服务器动态上下线：服务端启动时向ZooKeeper注册信息，客户端向ZooKeeper获取当前在线服务器列表并且注册监听。当服务器上下线时，ZooKeeper向<br>客户端发送事件通知，客户端在重新获取服务器列表并注册监听。<br>软负载均衡：通过在Zookeeper中记录每台服务器的访问次数，每次让访问次数最少的服务器去处理最新的客户端请求。</p><h2 id="ZooKeeper内部原理"><a href="#ZooKeeper内部原理" class="headerlink" title="ZooKeeper内部原理"></a>ZooKeeper内部原理</h2><h3 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h3><p><strong>半数机制：集群中半数以上机器存活，集群可用。因此ZooKeeper适合安装奇数台服务器</strong><br>Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。<br>假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201116154538893.png" alt="image-20201116154538893" title="">                </div>                <div class="image-caption">image-20201116154538893</div>            </figure><ol><li>服务器1启动，此时只有它一台服务器启动了，它发出去的报文没有任何响应，所以它的选举状态一直是LOOKING状态。</li><li>服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。</li><li>服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的Leader。</li><li>服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。</li><li>服务器5启动，同4一样当小弟。</li></ol><h3 id="节点类型"><a href="#节点类型" class="headerlink" title="节点类型"></a>节点类型</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201116155012746.png" alt="image-20201116155012746" title="">                </div>                <div class="image-caption">image-20201116155012746</div>            </figure><h3 id="Stat结构体"><a href="#Stat结构体" class="headerlink" title="Stat结构体"></a>Stat结构体</h3><ol><li><p>czxid-创建节点的事务zxid</p><p>每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</p><p>事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</p></li><li><p>ctime - znode被创建的毫秒数(从1970年开始)</p></li><li><p>mzxid - znode最后更新的事务zxid</p></li><li><p>mtime - znode最后修改的毫秒数(从1970年开始)</p></li><li><p>pZxid-znode最后更新的子节点zxid</p></li><li><p>cversion - znode子节点变化号，znode子节点修改次数</p></li><li><p>dataversion - znode数据变化号</p></li><li><p>aclVersion - znode访问控制列表的变化号</p></li><li><p>ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</p></li><li><p><strong>dataLength- znode的数据长度</strong></p></li><li><p><strong>numChildren - znode子节点数量</strong></p></li></ol><h3 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201116155707823.png" alt="image-20201116155707823" title="">                </div>                <div class="image-caption">image-20201116155707823</div>            </figure><h3 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/xdmqp/myPic/main/img/image-20201116160005237.png" alt="image-20201116160005237" title="">                </div>                <div class="image-caption">image-20201116160005237</div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ZooKeeper基础&quot;&gt;&lt;a href=&quot;#ZooKeeper基础&quot; class=&quot;headerlink&quot; title=&quot;ZooKeeper基础&quot;&gt;&lt;/a&gt;ZooKeeper基础&lt;/h1&gt;&lt;h2 id=&quot;入门&quot;&gt;&lt;a href=&quot;#入门&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://xdmqp.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="https://xdmqp.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="ZooKeeper" scheme="https://xdmqp.github.io/tags/ZooKeeper/"/>
    
  </entry>
  
</feed>
